# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä½“èŠå¤© API Blueprint
------------------------
æ¥å£ï¼š
  POST /api/agent/chat - æ™®é€šèŠå¤©æ¥å£
  POST /api/agent/chat/stream - æµå¼èŠå¤©æ¥å£
  GET /api/agent/chat/history - è·å–èŠå¤©è®°å½•
  DELETE /api/agent/chat/history/<session_id> - åˆ é™¤èŠå¤©ä¼šè¯

åŠŸèƒ½ï¼š
  - å‰åç«¯åˆ†ç¦»ï¼šLLM APIè°ƒç”¨åœ¨åç«¯å®Œæˆï¼Œæ”¯æŒå¤šç§æ¨¡å‹åç«¯
  - èŠå¤©è®°å½•æŒä¹…åŒ–ï¼šå­˜å‚¨åˆ°Supabase
  - æµå¼ä¼ è¾“ï¼šæ”¯æŒSSEæµå¼å“åº”
  - æ™ºèƒ½è·¯ç”±ï¼šè‡ªåŠ¨æˆ–æ‰‹åŠ¨é€‰æ‹©åˆé€‚çš„æœåŠ¡ï¼ˆGPT-Researcher/DeepAnalyze/Default LLMï¼‰
"""

import os
import json
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import requests
from flask import Blueprint, request, make_response, stream_with_context, jsonify

from infra.db import supabase
from infra.llm import call_volcano_chat as llm_chat

# å¯¼å…¥é…ç½®æ¨¡å—
from config import build_system_prompt, get_default_options

# å¯¼å…¥ GPT-Researcher é€‚é…å™¨
from backend_api.gpt_researcher_adapter import (
    get_gpt_researcher_adapter, 
    detect_task_type
)

# å¯¼å…¥ DeepAnalyze é€‚é…å™¨
from backend_api.deepanalyze_adapter import get_deepanalyze_adapter
from backend_api.rag.rag_search import run_semantic_retrieval
from backend_api.rag.rag_context import build_messages_with_evidence, build_evidence_block
from backend_api.web_search import (
    search_web,
    build_web_evidence_block,
)

# ===== é…ç½® =====
# GPT-Researcher é…ç½®
USE_GPT_RESEARCHER = os.getenv("USE_GPT_RESEARCHER", "true").lower() == "true"
AUTO_ROUTE_TASKS = os.getenv("AUTO_ROUTE_TASKS", "true").lower() == "true"  # æ˜¯å¦è‡ªåŠ¨è·¯ç”±ä»»åŠ¡

# DeepAnalyze é…ç½®
USE_DEEPANALYZE = os.getenv("USE_DEEPANALYZE", "true").lower() == "true"

# èŠå¤©è®°å½•è¡¨åï¼ˆéœ€è¦åœ¨Supabaseä¸­åˆ›å»ºï¼‰
CHAT_SESSIONS_TABLE = os.getenv("CHAT_SESSIONS_TABLE", "chat_sessions")
CHAT_MESSAGES_TABLE = os.getenv("CHAT_MESSAGES_TABLE", "chat_messages")

RAG_KEYWORDS = ["æ–°é—»", "äº‹ä»¶", "å‘å¸ƒ", "å…¬å‘Š", "åŠ¨æ€", "å‘ç”Ÿ", "æœ€è¿‘", "æœ€æ–°", "æœ‰å“ªäº›", "å˜åŒ–", "æ›´æ–°"]
WEB_SEARCH_KEYWORDS = [
    "æœ€æ–°",
    "æœ€è¿‘",
    "æ–°é—»",
    "æ”¿ç­–",
    "å‘å¸ƒ",
    "è¿›å±•",
    "ç ”ç©¶",
    "æŠ¥é“",
    "å…¬å‘Š",
    "è¡Œä¸š",
]

agent_chat_bp = Blueprint("agent_chat", __name__)
_supabase = supabase


def _get_env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, default))
    except Exception:
        return default


def _get_env_float(name: str, default: float) -> float:
    try:
        return float(os.getenv(name, default))
    except Exception:
        return default


USE_WEB_SEARCH = os.getenv("USE_WEB_SEARCH", "true").lower() == "true"
WEB_SEARCH_TOPK = _get_env_int("WEB_SEARCH_TOPK", 6)
WEB_SEARCH_CACHE_MINUTES = _get_env_int("WEB_SEARCH_CACHE_MINUTES", 30)
WEB_SEARCH_MIN_SCORE = _get_env_float("WEB_SEARCH_MIN_SCORE", 0.0)


def _to_iso(dt: Optional[datetime]) -> str:
    """å°†datetimeè½¬æ¢ä¸ºISO8601æ ¼å¼"""
    if dt is None:
        dt = datetime.now(timezone.utc)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _contains_keyword(text: str, keywords: List[str]) -> bool:
    if not text:
        return False
    return any(keyword in text for keyword in keywords)


def _build_sources_payload(evidence: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    grouped = {"database": [], "internet": []}
    seen = {"database": set(), "internet": set()}

    for item in evidence or []:
        if not isinstance(item, dict):
            continue
        origin = (item.get("origin") or "").strip().lower()
        if origin == "rag":
            group_key = "database"
        elif origin == "web":
            group_key = "internet"
        else:
            continue

        title = str(item.get("title") or "").strip()
        url = str(item.get("url") or "").strip()
        source_name = str(item.get("source") or "").strip()
        published_at = (
            item.get("published_at")
            or item.get("publishedAt")
            or item.get("published")
            or item.get("date")
            or ""
        )

        if not title and not url:
            continue
        key = url or title
        if key in seen[group_key]:
            continue
        seen[group_key].add(key)

        grouped[group_key].append(
            {
                "title": title or url or source_name or "æœªçŸ¥æ¥æº",
                "url": url,
                "source": source_name,
                "publishedAt": published_at,
                "origin": origin,
            }
        )

    return grouped


def _build_retrieval_messages(
    user_message: str,
    system_prompt: str,
    conversation_history: List[Dict[str, str]],
    use_rag: bool,
    use_web_search: bool,
) -> Dict[str, object]:
    rag_triggered = bool(use_rag) or _contains_keyword(user_message, RAG_KEYWORDS)
    web_triggered = bool(use_web_search) or _contains_keyword(user_message, WEB_SEARCH_KEYWORDS)
    combined_triggered = rag_triggered or web_triggered

    rag_results: List[Dict[str, Any]] = []
    web_results: List[Dict[str, object]] = []
    used_evidence: List[Dict[str, Any]] = []

    if combined_triggered:
        try:
            rag_results = run_semantic_retrieval(
                user_message,
                k=_get_env_int("RAG_TOPK", 8),
                min_sim=_get_env_float("RAG_MIN_SIM", 0.4),
            )
        except Exception as exc:
            print(f"âš ï¸ RAG å¤„ç†å¼‚å¸¸ï¼Œå›é€€åˆ°é»˜è®¤èŠå¤©: {exc}")

        if USE_WEB_SEARCH:
            try:
                web_results = search_web(
                    user_message,
                    max_results=WEB_SEARCH_TOPK,
                    min_score=WEB_SEARCH_MIN_SCORE,
                    cache_ttl_seconds=WEB_SEARCH_CACHE_MINUTES * 60,
                )
            except Exception as exc:
                print(f"âš ï¸ Web æœç´¢å¤±è´¥: {exc}")
        elif web_triggered:
            print("âš ï¸ Web æœç´¢å·²è§¦å‘ä½†æœªå¯ç”¨ USE_WEB_SEARCH")

    if rag_results:
        used_evidence.extend(
            [
                {
                    "id": item.get("id"),
                    "title": item.get("title"),
                    "summary": item.get("summary"),
                    "url": item.get("url"),
                    "published_at": item.get("published_at"),
                    "source": item.get("source") or item.get("source_name"),
                    "similarity": item.get("similarity"),
                    "origin": "rag",
                }
                for item in rag_results
            ]
        )
    if web_results:
        used_evidence.extend(
            [
                {
                    "id": None,
                    "title": item.get("title"),
                    "summary": item.get("snippet"),
                    "url": item.get("url"),
                    "published_at": item.get("publishedAt"),
                    "source": item.get("source"),
                    "similarity": item.get("score"),
                    "origin": "web",
                }
                for item in web_results
            ]
        )

    evidence_blocks: List[str] = []
    if rag_results:
        evidence_blocks.append(f"ã€æœ¬åœ°çŸ¥è¯†åº“ã€‘\n{build_evidence_block(rag_results)}")
    if web_results:
        evidence_blocks.append(build_web_evidence_block(web_results))

    messages: List[Dict[str, str]] = []
    if evidence_blocks:
        system_content = system_prompt or "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"
        system_content = (
            f"{system_content}\n\n"
            "è¯·ä¼˜å…ˆä½¿ç”¨ã€æœ¬åœ°çŸ¥è¯†åº“ã€‘è¯æ®å›ç­”ï¼Œå¿…è¦æ—¶å†å¼•ç”¨ã€ç½‘ç»œæœç´¢ã€‘ï¼›"
            "ä¸è¦åœ¨æ­£æ–‡ä¸­è¾“å‡ºæ¥æºåˆ—è¡¨ï¼Œæ¥æºç”±ç³»ç»Ÿç»Ÿä¸€è¿½åŠ ã€‚"
            "è‹¥è¯æ®ä¸è¶³ï¼Œè¯·ç›´æ¥è¯´æ˜ä¿¡æ¯ä¸è¶³ï¼Œä¸è¦ç¼–é€ ã€‚\n"
            + "\n\n".join(evidence_blocks)
        )
        messages = [{"role": "system", "content": system_content}]
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})

        if rag_results:
            print(f"ğŸ” RAG æ£€ç´¢å·²è§¦å‘ï¼Œè¿”å› {len(rag_results)} æ¡è¯æ®")
        if web_results:
            print(f"ğŸŒ Web æœç´¢å·²è§¦å‘ï¼Œè¿”å› {len(web_results)} æ¡è¯æ®")
    else:
        if combined_triggered:
            print("ğŸ” æ£€ç´¢å·²è§¦å‘ä½†æ— è¯æ®ï¼Œå›é€€åˆ°é»˜è®¤æç¤º")

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})

    return {
        "messages": messages,
        "used_evidence": used_evidence,
        "sources": _build_sources_payload(used_evidence),
    }


def _call_default_llm(messages: List[Dict[str, str]], stream: bool = False, **options):
    """
    è°ƒç”¨é»˜è®¤çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
    
    è¯¥å‡½æ•°æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ï¼Œå®é™…ä½¿ç”¨çš„æ¨¡å‹ç”± DEFAULT_LLM_PROVIDER 
    ç¯å¢ƒå˜é‡æ§åˆ¶ï¼ˆå¦‚ volcano, qwen, gpt-4, claude ç­‰ï¼‰ã€‚
    è¿™æ ·è®¾è®¡èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ä»£ç çš„æƒ…å†µä¸‹çµæ´»åˆ‡æ¢åº•å±‚ LLM æœåŠ¡ã€‚
    """
    return llm_chat(messages, stream=stream, **options)


def _call_llm_api(
    messages: List[Dict[str, str]], 
    user_message: str = "",
    stream: bool = False,
    force_provider: str = None,
    **options
):
    """
    ç»Ÿä¸€çš„ LLM API è°ƒç”¨å‡½æ•°ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœåŠ¡
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºä»»åŠ¡ç±»å‹æ£€æµ‹ï¼‰
        stream: æ˜¯å¦æµå¼å“åº”
        force_provider: å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæœåŠ¡ ('qwen', 'gpt-researcher', 'deepanalyze', 'auto')
        **options: å…¶ä»–é€‰é¡¹
    
    Returns:
        å“åº”å¯¹è±¡æˆ–ç”Ÿæˆå™¨
    """
    # å¦‚æœå¼ºåˆ¶æŒ‡å®šäº†æœåŠ¡
    if force_provider == 'gpt-researcher':
        adapter = get_gpt_researcher_adapter()
        if stream:
            return adapter.chat_completions_stream(messages, **options)
        else:
            result = adapter.chat_completions(messages, **options)
            # è½¬æ¢ä¸º requests.Response å…¼å®¹æ ¼å¼
            class MockResponse:
                def __init__(self, data):
                    self._data = data
                def json(self):
                    return self._data
                def raise_for_status(self):
                    pass
            return MockResponse(result)
    
    if force_provider == 'deepanalyze':
        adapter = get_deepanalyze_adapter()
        if stream:
            return adapter.chat_completions_stream(messages, **options)
        else:
            result = adapter.chat_completions(messages, **options)
            class MockResponse:
                def __init__(self, data):
                    self._data = data
                def json(self):
                    return self._data
                def raise_for_status(self):
                    pass
            return MockResponse(result)
    
    # è‡ªåŠ¨è·¯ç”±æˆ–ä½¿ç”¨ Qwen
    if AUTO_ROUTE_TASKS and (USE_GPT_RESEARCHER or USE_DEEPANALYZE) and user_message:
        task_type = detect_task_type(user_message)
        if task_type == 'research' and USE_GPT_RESEARCHER:
            # ç ”ç©¶ä»»åŠ¡ä½¿ç”¨ GPT-Researcher
            adapter = get_gpt_researcher_adapter()
            if stream:
                return adapter.chat_completions_stream(messages, **options)
            else:
                result = adapter.chat_completions(messages, **options)
                class MockResponse:
                    def __init__(self, data):
                        self._data = data
                    def json(self):
                        return self._data
                    def raise_for_status(self):
                        pass
                return MockResponse(result)
        elif task_type == 'data' and USE_DEEPANALYZE:
            # æ•°æ®åˆ†æä»»åŠ¡ä½¿ç”¨ DeepAnalyze
            adapter = get_deepanalyze_adapter()
            if stream:
                return adapter.chat_completions_stream(messages, **options)
            else:
                result = adapter.chat_completions(messages, **options)
                class MockResponse:
                    def __init__(self, data):
                        self._data = data
                    def json(self):
                        return self._data
                    def raise_for_status(self):
                        pass
                return MockResponse(result)
    
    # é»˜è®¤ä½¿ç”¨æœ¬åœ° LLM æœåŠ¡
    return _call_default_llm(messages, stream=stream, **options)


def _save_message(session_id: str, role: str, content: str, message_id: Optional[str] = None):
    """ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    if not _supabase:
        return None
    
    try:
        message_data = {
            "session_id": session_id,
            "role": role,
            "content": content,
            "created_at": _to_iso(datetime.now(timezone.utc)),
        }
        
        if message_id:
            message_data["id"] = message_id
        
        result = _supabase.table(CHAT_MESSAGES_TABLE).insert(message_data).execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")
        return None


def _create_or_update_session(session_id: str, title: Optional[str] = None):
    """åˆ›å»ºæˆ–æ›´æ–°èŠå¤©ä¼šè¯"""
    if not _supabase:
        return None
    
    try:
        now = _to_iso(datetime.now(timezone.utc))
        session_data = {
            "id": session_id,
            "title": title or "æ–°å¯¹è¯",
            "updated_at": now,
        }
        
        # å…ˆæŸ¥è¯¢ä¼šè¯æ˜¯å¦å­˜åœ¨
        existing = _supabase.table(CHAT_SESSIONS_TABLE).select("id").eq("id", session_id).execute()
        
        if existing.data and len(existing.data) > 0:
            # ä¼šè¯å­˜åœ¨ï¼Œæ›´æ–°
            result = _supabase.table(CHAT_SESSIONS_TABLE).update(session_data).eq("id", session_id).execute()
            return result.data[0] if result.data else None
        else:
            # ä¼šè¯ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°ä¼šè¯
            session_data["created_at"] = now
            result = _supabase.table(CHAT_SESSIONS_TABLE).insert(session_data).execute()
            return result.data[0] if result.data else None
    except Exception as e:
        print(f"âš ï¸ åˆ›å»º/æ›´æ–°ä¼šè¯å¤±è´¥: {e}")
        return None


def _get_chat_history(session_id: str, limit: int = 50) -> List[Dict[str, Any]]:
    """è·å–èŠå¤©å†å²è®°å½•"""
    if not _supabase:
        return []
    
    try:
        result = (
            _supabase.table(CHAT_MESSAGES_TABLE)
            .select("*")
            .eq("session_id", session_id)
            .order("created_at", desc=False)
            .limit(limit)
            .execute()
        )
        return result.data or []
    except Exception as e:
        print(f"âš ï¸ è·å–èŠå¤©å†å²å¤±è´¥: {e}")
        return []


@agent_chat_bp.route("/chat", methods=["POST"])
def chat():
    """æ™®é€šèŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()
        session_id = data.get("session_id") or str(uuid.uuid4())
        
        # ä»é…ç½®æ–‡ä»¶è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å†ä»å‰ç«¯ä¼ é€’ï¼‰
        temporary_prompts = data.get("temporary_prompts", [])  # å‰ç«¯å¯ä»¥ä¼ é€’ä¸´æ—¶æç¤ºè¯
        system_prompt = build_system_prompt(temporary_prompts=temporary_prompts)
        
        conversation_history = data.get("conversation_history", [])
        options = data.get("options", {})
        
        # åˆå¹¶é»˜è®¤é€‰é¡¹
        default_options = get_default_options()
        options = {**default_options, **options}
        
        if not user_message:
            return jsonify({"code": 400, "message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º", "data": None}), 400
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆæœ¬åœ° RAG + è”ç½‘æœç´¢ï¼‰
        use_rag = data.get("use_rag", False)
        use_web_search = data.get("use_web_search", False)
        retrieval = _build_retrieval_messages(
            user_message=user_message,
            system_prompt=system_prompt,
            conversation_history=conversation_history,
            use_rag=use_rag,
            use_web_search=use_web_search,
        )
        messages = retrieval["messages"]
        used_evidence = retrieval["used_evidence"]
        sources_payload = retrieval["sources"]
        
        # å…ˆåˆ›å»ºæˆ–æ›´æ–°ä¼šè¯ï¼ˆç¡®ä¿ä¼šè¯å­˜åœ¨ï¼‰
        _create_or_update_session(session_id)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        _save_message(session_id, "user", user_message)
        
        # è·å–ä»»åŠ¡ç±»å‹ï¼ˆä»å‰ç«¯ä¼ é€’æˆ–è‡ªåŠ¨æ£€æµ‹ï¼‰
        task_type = data.get("task_type", "auto")
        
        # æ ¹æ® task_type å†³å®šä½¿ç”¨å“ªä¸ªæœåŠ¡
        force_provider = None
        if task_type == 'research':
            force_provider = 'gpt-researcher'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ GPT-Researcherï¼ˆç ”ç©¶ä»»åŠ¡ï¼‰")
        elif task_type == 'data':
            force_provider = 'deepanalyze'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ DeepAnalyzeï¼ˆæ•°æ®åˆ†æä»»åŠ¡ï¼‰")
        elif task_type == 'chat':
            force_provider = 'qwen'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ Qwenï¼ˆèŠå¤©ä»»åŠ¡ï¼‰")
        else:
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] ä½¿ç”¨è‡ªåŠ¨è·¯ç”±ï¼ˆtask_type: {task_type}ï¼‰")
        
        # è°ƒç”¨ LLM APIï¼ˆæ ¹æ® task_type é€‰æ‹©æœåŠ¡ï¼‰
        response = _call_llm_api(messages, user_message=user_message, stream=False, force_provider=force_provider, **options)
        result = response.json()
        
        # æå–AIå›å¤
        ai_content = ""
        if result.get("choices") and len(result["choices"]) > 0:
            ai_content = result["choices"][0].get("message", {}).get("content", "")
        elif result.get("output"):
            output = result["output"]
            if output.get("text"):
                ai_content = output["text"]
            elif output.get("choices") and len(output["choices"]) > 0:
                ai_content = output["choices"][0].get("message", {}).get("content", "") or output["choices"][0].get("text", "")
        
        if not ai_content:
            ai_content = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·æ¢ä¸ªæ–¹å¼æé—®ã€‚"

        # ä¿å­˜AIå›å¤
        _save_message(session_id, "assistant", ai_content)
        
        # æ›´æ–°ä¼šè¯
        _create_or_update_session(session_id)
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {
                "session_id": session_id,
                "content": ai_content,
                "evidence": used_evidence,  # NEW: è¿”å›è¯æ®
                "sources": sources_payload,
                "conversation_history": conversation_history + [
                    {"role": "user", "content": user_message},
                    {"role": "assistant", "content": ai_content}
                ]
            }
        })
        
    except requests.exceptions.RequestException as e:
        error_msg = f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
        if hasattr(e, "response") and e.response is not None:
            try:
                error_data = e.response.json()
                error_msg = error_data.get("message", error_msg)
            except:
                pass
        return jsonify({"code": 500, "message": error_msg, "data": None}), 500
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/stream", methods=["POST"])
def chat_stream():
    """æµå¼èŠå¤©æ¥å£ï¼ˆSSEï¼‰"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()
        session_id = data.get("session_id") or str(uuid.uuid4())
        
        # ä»é…ç½®æ–‡ä»¶è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å†ä»å‰ç«¯ä¼ é€’ï¼‰
        temporary_prompts = data.get("temporary_prompts", [])  # å‰ç«¯å¯ä»¥ä¼ é€’ä¸´æ—¶æç¤ºè¯
        system_prompt = build_system_prompt(temporary_prompts=temporary_prompts)
        
        conversation_history = data.get("conversation_history", [])
        options = data.get("options", {})
        task_type = data.get("task_type", "auto")  # ä»»åŠ¡ç±»å‹ï¼š'research' å¼ºåˆ¶ä½¿ç”¨ GPT-Researcher, 'chat' ä½¿ç”¨ Qwen, 'auto' è‡ªåŠ¨è·¯ç”±
        
        # åˆå¹¶é»˜è®¤é€‰é¡¹
        default_options = get_default_options()
        options = {**default_options, **options}
        
        if not user_message:
            return jsonify({"code": 400, "message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º", "data": None}), 400
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆæœ¬åœ° RAG + è”ç½‘æœç´¢ï¼‰
        use_rag = data.get("use_rag", False)
        use_web_search = data.get("use_web_search", False)
        retrieval = _build_retrieval_messages(
            user_message=user_message,
            system_prompt=system_prompt,
            conversation_history=conversation_history,
            use_rag=use_rag,
            use_web_search=use_web_search,
        )
        messages = retrieval["messages"]
        used_evidence = retrieval["used_evidence"]
        
        # å…ˆåˆ›å»ºæˆ–æ›´æ–°ä¼šè¯ï¼ˆç¡®ä¿ä¼šè¯å­˜åœ¨ï¼‰
        _create_or_update_session(session_id)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_message_id = str(uuid.uuid4())
        _save_message(session_id, "user", user_message, user_message_id)
        
        def generate():
            """ç”Ÿæˆæµå¼å“åº”"""
            ai_content = ""
            ai_message_id = str(uuid.uuid4())
            
            try:
                # æ ¹æ® task_type å†³å®šä½¿ç”¨å“ªä¸ªæœåŠ¡
                use_gpt_researcher = False
                use_deepanalyze = False
                force_provider = None
                
                if task_type == 'research':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ GPT-Researcher
                    use_gpt_researcher = True
                    force_provider = 'gpt-researcher'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ GPT-Researcherï¼ˆç ”ç©¶ä»»åŠ¡ï¼‰")
                elif task_type == 'data':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ DeepAnalyze
                    use_deepanalyze = True
                    force_provider = 'deepanalyze'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ DeepAnalyzeï¼ˆæ•°æ®åˆ†æä»»åŠ¡ï¼‰")
                elif task_type == 'chat':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ Qwen
                    force_provider = 'qwen'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ Qwenï¼ˆèŠå¤©ä»»åŠ¡ï¼‰")
                elif AUTO_ROUTE_TASKS and (USE_GPT_RESEARCHER or USE_DEEPANALYZE) and user_message:
                    # è‡ªåŠ¨è·¯ç”±
                    detected_task_type = detect_task_type(user_message)
                    if detected_task_type == 'research' and USE_GPT_RESEARCHER:
                        use_gpt_researcher = True
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°ç ”ç©¶ä»»åŠ¡ï¼Œè·¯ç”±åˆ° GPT-Researcher")
                        print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
                    elif detected_task_type == 'data' and USE_DEEPANALYZE:
                        use_deepanalyze = True
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®åˆ†æä»»åŠ¡ï¼Œè·¯ç”±åˆ° DeepAnalyze")
                        print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
                    else:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°{detected_task_type}ä»»åŠ¡ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡ (Qwen)")
                else:
                    if not AUTO_ROUTE_TASKS:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨è·¯ç”±å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡")
                    elif not USE_GPT_RESEARCHER and not USE_DEEPANALYZE:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] GPT-Researcher å’Œ DeepAnalyze å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡")
                
                # å‘é€åˆå§‹äº‹ä»¶
                yield f"data: {json.dumps({'type': 'start', 'session_id': session_id}, ensure_ascii=False)}\n\n"
                # NEW: æ¨é€è¯æ®äº‹ä»¶ï¼ˆç©ºåˆ—è¡¨ä¹Ÿå‘é€ï¼Œæ–¹ä¾¿å‰ç«¯å¤„ç†ï¼‰
                if used_evidence:
                    yield f"data: {json.dumps({'type': 'evidence', 'items': used_evidence}, ensure_ascii=False)}\n\n"
                elif used_evidence == []:
                    yield f"data: {json.dumps({'type': 'evidence', 'items': []}, ensure_ascii=False)}\n\n"
                
                if use_gpt_researcher:
                    # ä½¿ç”¨ GPT-Researcherï¼ˆæ”¯æŒè¿›åº¦æ˜¾ç¤ºï¼‰
                    print(f"ğŸš€ [GPT-Researcher] å¼€å§‹è°ƒç”¨ç ”ç©¶æœåŠ¡...")
                    adapter = get_gpt_researcher_adapter()
                    
                    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆç”¨äºå‘é€è¿›åº¦åˆ°å‰ç«¯ï¼‰
                    progress_queue = []
                    def progress_callback(progress_data):
                        """å°†è¿›åº¦ä¿¡æ¯æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œç¨åé€šè¿‡ SSE å‘é€"""
                        progress_queue.append(progress_data)
                    
                    chunk_count = 0
                    for chunk in adapter.chat_completions_stream(
                        messages, 
                        progress_callback=progress_callback,
                        **options
                    ):
                        # å…ˆå‘é€æ‰€æœ‰ç´¯ç§¯çš„è¿›åº¦ä¿¡æ¯
                        while progress_queue:
                            progress_data = progress_queue.pop(0)
                            progress_json = json.dumps(progress_data, ensure_ascii=False)
                            yield f"data: {progress_json}\n\n"
                        
                        # å¤„ç†å†…å®¹å—
                        content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if content:
                            ai_content += content
                            chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                            yield f"data: {chunk_data}\n\n"
                            chunk_count += 1
                            # åªåœ¨å‰å‡ ä¸ªchunkæ‰“å°æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            if chunk_count <= 3:
                                print(f"ğŸ“¤ [GPT-Researcher] å‘é€chunk #{chunk_count}: {content[:30]}...")
                    
                    # å‘é€å‰©ä½™çš„è¿›åº¦ä¿¡æ¯
                    while progress_queue:
                        progress_data = progress_queue.pop(0)
                        progress_json = json.dumps(progress_data, ensure_ascii=False)
                        yield f"data: {progress_json}\n\n"
                    
                    print(f"âœ… [GPT-Researcher] å®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦: {len(ai_content)} å­—ç¬¦")
                elif use_deepanalyze:
                    # ä½¿ç”¨ DeepAnalyze APIï¼ˆåŸç”Ÿæµå¼æ”¯æŒï¼‰
                    print(f"ğŸš€ [DeepAnalyze] å¼€å§‹è°ƒç”¨æ•°æ®åˆ†ææœåŠ¡...")
                    adapter = get_deepanalyze_adapter()
                    
                    chunk_count = 0
                    for chunk in adapter.chat_completions_stream(messages, **options):
                        # å¤„ç†å†…å®¹å—
                        content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if content:
                            ai_content += content
                            chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                            yield f"data: {chunk_data}\n\n"
                            chunk_count += 1
                            # åªåœ¨å‰å‡ ä¸ªchunkæ‰“å°æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            if chunk_count <= 3:
                                print(f"ğŸ“¤ [DeepAnalyze] å‘é€chunk #{chunk_count}: {content[:30]}...")
                    
                    print(f"âœ… [DeepAnalyze] å®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦: {len(ai_content)} å­—ç¬¦")
                else:
                    # ä½¿ç”¨ Qwen APIï¼ˆçœŸæ­£çš„æµå¼ï¼‰
                    response = _call_llm_api(messages, user_message=user_message, stream=True, force_provider=force_provider, **options)
                    
                    # å¤„ç†æµå¼å“åº”
                    for line in response.iter_lines():
                        if not line:
                            continue
                        
                        line_str = line.decode("utf-8")
                        
                        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                        if not line_str.strip() or line_str.startswith(':'):
                            continue
                        
                        # ç§»é™¤ "data: " å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if line_str.startswith("data: "):
                            line_str = line_str[6:]
                        
                        # æ£€æŸ¥ç»“æŸæ ‡è®°
                        if line_str.strip() == "[DONE]" or line_str.strip() == 'data:[DONE]':
                            break
                        
                        try:
                            chunk_data = json.loads(line_str)
                            
                            # å°è¯•å¤šç§æ ¼å¼è§£æ
                            content = ""
                            
                            # OpenAIå…¼å®¹æ ¼å¼
                            if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                                delta = chunk_data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                            
                            # DashScopeæ ¼å¼ï¼ˆQwenå¯èƒ½ä½¿ç”¨ï¼‰
                            elif "output" in chunk_data:
                                output = chunk_data["output"]
                                if "choices" in output and len(output["choices"]) > 0:
                                    choice = output["choices"][0]
                                    if "delta" in choice:
                                        content = choice["delta"].get("content", "")
                                    elif "message" in choice:
                                        content = choice["message"].get("content", "")
                                    elif "text" in choice:
                                        content = choice.get("text", "")
                                elif "text" in output:
                                    content = output.get("text", "")
                            
                            # ç›´æ¥æ–‡æœ¬æ ¼å¼
                            elif "text" in chunk_data:
                                content = chunk_data.get("text", "")
                            
                            # å¦‚æœæ‰¾åˆ°å†…å®¹ï¼Œç«‹å³å‘é€
                            if content:
                                ai_content += content
                                # ç«‹å³flushï¼Œç¡®ä¿å®æ—¶ä¼ è¾“
                                chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                                yield f"data: {chunk_data}\n\n"
                                # è°ƒè¯•ï¼šæ‰“å°å‘é€çš„chunkï¼ˆä»…å‰å‡ ä¸ªå­—ç¬¦ï¼‰
                                if len(ai_content) <= 50:
                                    print(f"ğŸ“¤ [Qwen] å‘é€chunk: {content[:20]}...")
                                
                        except json.JSONDecodeError as e:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬ï¼Œè·³è¿‡
                            print(f"âš ï¸ è§£ææµå¼æ•°æ®å¤±è´¥: {e}, è¡Œå†…å®¹: {line_str[:100]}")
                            continue
                        except Exception as e:
                            print(f"âš ï¸ å¤„ç†æµå¼æ•°æ®å‡ºé”™: {e}")
                            continue
                
                # å‘é€å®Œæˆäº‹ä»¶
                yield f"data: {json.dumps({'type': 'done', 'session_id': session_id}, ensure_ascii=False)}\n\n"
                
                # ä¿å­˜å®Œæ•´çš„AIå›å¤
                if ai_content:
                    _save_message(session_id, "assistant", ai_content, ai_message_id)
                    _create_or_update_session(session_id)
                else:
                    # å¦‚æœæ²¡æœ‰æ”¶åˆ°å†…å®¹ï¼Œä¿å­˜é»˜è®¤æ¶ˆæ¯
                    default_msg = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·æ¢ä¸ªæ–¹å¼æé—®ã€‚"
                    _save_message(session_id, "assistant", default_msg, ai_message_id)
                    yield f"data: {json.dumps({'type': 'error', 'message': default_msg}, ensure_ascii=False)}\n\n"
                    
            except Exception as e:
                error_msg = f"æµå¼ä¼ è¾“é”™è¯¯: {str(e)}"
                yield f"data: {json.dumps({'type': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
        
        response = make_response(stream_with_context(generate()))
        response.headers["Content-Type"] = "text/event-stream; charset=utf-8"
        response.headers["Cache-Control"] = "no-cache, no-transform"
        response.headers["Connection"] = "keep-alive"
        response.headers["X-Accel-Buffering"] = "no"  # ç¦ç”¨Nginxç¼“å†²
        return response
        
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/history", methods=["GET"])
def get_chat_history():
    """è·å–èŠå¤©è®°å½•"""
    try:
        session_id = request.args.get("session_id")
        limit = int(request.args.get("limit", 50))
        
        if not session_id:
            return jsonify({"code": 400, "message": "session_idå‚æ•°å¿…å¡«", "data": None}), 400
        
        history = _get_chat_history(session_id, limit)
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        messages = []
        for msg in history:
            messages.append({
                "role": msg.get("role"),
                "content": msg.get("content"),
                "time": msg.get("created_at", ""),
            })
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {
                "session_id": session_id,
                "messages": messages
            }
        })
        
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/sessions", methods=["GET"])
def get_chat_sessions():
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯åˆ—è¡¨"""
    if not _supabase:
        return jsonify({"code": 200, "message": "success", "data": {"sessions": []}})
    
    try:
        limit = int(request.args.get("limit", 20))
        result = (
            _supabase.table(CHAT_SESSIONS_TABLE)
            .select("*")
            .order("updated_at", desc=True)
            .limit(limit)
            .execute()
        )
        
        sessions = result.data or []
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {"sessions": sessions}
        })
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/sessions/<session_id>", methods=["DELETE"])
def delete_chat_session(session_id):
    """åˆ é™¤èŠå¤©ä¼šè¯åŠå…¶æ‰€æœ‰æ¶ˆæ¯"""
    if not _supabase:
        return jsonify({"code": 200, "message": "success", "data": None})
    
    try:
        # åˆ é™¤æ¶ˆæ¯
        _supabase.table(CHAT_MESSAGES_TABLE).delete().eq("session_id", session_id).execute()
        # åˆ é™¤ä¼šè¯
        _supabase.table(CHAT_SESSIONS_TABLE).delete().eq("id", session_id).execute()
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": None
        })
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500
