# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä½“èŠå¤© API Blueprint
------------------------
æ¥å£ï¼š
  POST /api/agent/chat - æ™®é€šèŠå¤©æ¥å£
  POST /api/agent/chat/stream - æµå¼èŠå¤©æ¥å£
  GET /api/agent/chat/history - è·å–èŠå¤©è®°å½•
  DELETE /api/agent/chat/history/<session_id> - åˆ é™¤èŠå¤©ä¼šè¯

åŠŸèƒ½ï¼š
  - å‰åç«¯åˆ†ç¦»ï¼šQwen APIè°ƒç”¨åœ¨åç«¯å®Œæˆ
  - èŠå¤©è®°å½•æŒä¹…åŒ–ï¼šå­˜å‚¨åˆ°Supabase
  - æµå¼ä¼ è¾“ï¼šæ”¯æŒSSEæµå¼å“åº”
"""

import os
import json
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import requests
from flask import Blueprint, request, make_response, stream_with_context, jsonify
from supabase import Client, create_client

# å¯¼å…¥é…ç½®æ¨¡å—
from config import build_system_prompt, get_default_options

# å¯¼å…¥ GPT-Researcher é€‚é…å™¨
from backend_api.gpt_researcher_adapter import (
    get_gpt_researcher_adapter, 
    detect_task_type
)

# å¯¼å…¥ DeepAnalyze é€‚é…å™¨
from backend_api.deepanalyze_adapter import get_deepanalyze_adapter

# ===== é…ç½® =====
QWEN_API_KEY = os.getenv("QWEN_API_KEY", "sk-7cd135dca0834256a58e960048238db3")
QWEN_BASE_URL = os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
QWEN_MODEL = os.getenv("QWEN_MODEL", "qwen-turbo")

# GPT-Researcher é…ç½®
USE_GPT_RESEARCHER = os.getenv("USE_GPT_RESEARCHER", "true").lower() == "true"
AUTO_ROUTE_TASKS = os.getenv("AUTO_ROUTE_TASKS", "true").lower() == "true"  # æ˜¯å¦è‡ªåŠ¨è·¯ç”±ä»»åŠ¡

# DeepAnalyze é…ç½®
USE_DEEPANALYZE = os.getenv("USE_DEEPANALYZE", "true").lower() == "true"

SUPABASE_URL = os.getenv("SUPABASE_URL", "https://zlajhzeylrzfbchycqyy.supabase.co")
SUPABASE_KEY = os.getenv(
    "SUPABASE_SERVICE_KEY",
    "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpsYWpoemV5bHJ6ZmJjaHljcXl5Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NTYwMTIwMiwiZXhwIjoyMDcxMTc3MjAyfQ.u6vYYEL3qCh4lJU62wEmT4UJTZrstX-_yscRPXrZH7s",
)

# èŠå¤©è®°å½•è¡¨åï¼ˆéœ€è¦åœ¨Supabaseä¸­åˆ›å»ºï¼‰
CHAT_SESSIONS_TABLE = os.getenv("CHAT_SESSIONS_TABLE", "chat_sessions")
CHAT_MESSAGES_TABLE = os.getenv("CHAT_MESSAGES_TABLE", "chat_messages")

agent_chat_bp = Blueprint("agent_chat", __name__)

# åˆå§‹åŒ–Supabaseå®¢æˆ·ç«¯
_supabase: Optional[Client] = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        _supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    except Exception as e:
        print(f"âš ï¸ Supabaseåˆå§‹åŒ–å¤±è´¥: {e}")
        _supabase = None


def _to_iso(dt: Optional[datetime]) -> str:
    """å°†datetimeè½¬æ¢ä¸ºISO8601æ ¼å¼"""
    if dt is None:
        dt = datetime.now(timezone.utc)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _call_qwen_api(messages: List[Dict[str, str]], stream: bool = False, **options):
    """è°ƒç”¨Qwen API"""
    url = f"{QWEN_BASE_URL}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {QWEN_API_KEY}",
    }
    
    data = {
        "model": QWEN_MODEL,
        "messages": messages,
        "temperature": options.get("temperature", 0.8),
        "top_p": options.get("top_p", 0.8),
        **({"stream": True} if stream else {}),
    }
    
    if options.get("max_tokens"):
        data["max_tokens"] = options["max_tokens"]
    
    response = requests.post(url, headers=headers, json=data, stream=stream, timeout=60)
    response.raise_for_status()
    return response


def _call_llm_api(
    messages: List[Dict[str, str]], 
    user_message: str = "",
    stream: bool = False,
    force_provider: str = None,
    **options
):
    """
    ç»Ÿä¸€çš„ LLM API è°ƒç”¨å‡½æ•°ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœåŠ¡
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºä»»åŠ¡ç±»å‹æ£€æµ‹ï¼‰
        stream: æ˜¯å¦æµå¼å“åº”
        force_provider: å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šæœåŠ¡ ('qwen', 'gpt-researcher', 'deepanalyze', 'auto')
        **options: å…¶ä»–é€‰é¡¹
    
    Returns:
        å“åº”å¯¹è±¡æˆ–ç”Ÿæˆå™¨
    """
    # å¦‚æœå¼ºåˆ¶æŒ‡å®šäº†æœåŠ¡
    if force_provider == 'gpt-researcher':
        adapter = get_gpt_researcher_adapter()
        if stream:
            return adapter.chat_completions_stream(messages, **options)
        else:
            result = adapter.chat_completions(messages, **options)
            # è½¬æ¢ä¸º requests.Response å…¼å®¹æ ¼å¼
            class MockResponse:
                def __init__(self, data):
                    self._data = data
                def json(self):
                    return self._data
                def raise_for_status(self):
                    pass
            return MockResponse(result)
    
    if force_provider == 'deepanalyze':
        adapter = get_deepanalyze_adapter()
        if stream:
            return adapter.chat_completions_stream(messages, **options)
        else:
            result = adapter.chat_completions(messages, **options)
            class MockResponse:
                def __init__(self, data):
                    self._data = data
                def json(self):
                    return self._data
                def raise_for_status(self):
                    pass
            return MockResponse(result)
    
    # è‡ªåŠ¨è·¯ç”±æˆ–ä½¿ç”¨ Qwen
    if AUTO_ROUTE_TASKS and (USE_GPT_RESEARCHER or USE_DEEPANALYZE) and user_message:
        task_type = detect_task_type(user_message)
        if task_type == 'research' and USE_GPT_RESEARCHER:
            # ç ”ç©¶ä»»åŠ¡ä½¿ç”¨ GPT-Researcher
            adapter = get_gpt_researcher_adapter()
            if stream:
                return adapter.chat_completions_stream(messages, **options)
            else:
                result = adapter.chat_completions(messages, **options)
                class MockResponse:
                    def __init__(self, data):
                        self._data = data
                    def json(self):
                        return self._data
                    def raise_for_status(self):
                        pass
                return MockResponse(result)
        elif task_type == 'data' and USE_DEEPANALYZE:
            # æ•°æ®åˆ†æä»»åŠ¡ä½¿ç”¨ DeepAnalyze
            adapter = get_deepanalyze_adapter()
            if stream:
                return adapter.chat_completions_stream(messages, **options)
            else:
                result = adapter.chat_completions(messages, **options)
                class MockResponse:
                    def __init__(self, data):
                        self._data = data
                    def json(self):
                        return self._data
                    def raise_for_status(self):
                        pass
                return MockResponse(result)
    
    # é»˜è®¤ä½¿ç”¨ Qwen
    return _call_qwen_api(messages, stream=stream, **options)


def _save_message(session_id: str, role: str, content: str, message_id: Optional[str] = None):
    """ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    if not _supabase:
        return None
    
    try:
        message_data = {
            "session_id": session_id,
            "role": role,
            "content": content,
            "created_at": _to_iso(datetime.now(timezone.utc)),
        }
        
        if message_id:
            message_data["id"] = message_id
        
        result = _supabase.table(CHAT_MESSAGES_TABLE).insert(message_data).execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")
        return None


def _create_or_update_session(session_id: str, title: Optional[str] = None):
    """åˆ›å»ºæˆ–æ›´æ–°èŠå¤©ä¼šè¯"""
    if not _supabase:
        return None
    
    try:
        now = _to_iso(datetime.now(timezone.utc))
        session_data = {
            "id": session_id,
            "title": title or "æ–°å¯¹è¯",
            "updated_at": now,
        }
        
        # å…ˆæŸ¥è¯¢ä¼šè¯æ˜¯å¦å­˜åœ¨
        existing = _supabase.table(CHAT_SESSIONS_TABLE).select("id").eq("id", session_id).execute()
        
        if existing.data and len(existing.data) > 0:
            # ä¼šè¯å­˜åœ¨ï¼Œæ›´æ–°
            result = _supabase.table(CHAT_SESSIONS_TABLE).update(session_data).eq("id", session_id).execute()
            return result.data[0] if result.data else None
        else:
            # ä¼šè¯ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°ä¼šè¯
            session_data["created_at"] = now
            result = _supabase.table(CHAT_SESSIONS_TABLE).insert(session_data).execute()
            return result.data[0] if result.data else None
    except Exception as e:
        print(f"âš ï¸ åˆ›å»º/æ›´æ–°ä¼šè¯å¤±è´¥: {e}")
        return None


def _get_chat_history(session_id: str, limit: int = 50) -> List[Dict[str, Any]]:
    """è·å–èŠå¤©å†å²è®°å½•"""
    if not _supabase:
        return []
    
    try:
        result = (
            _supabase.table(CHAT_MESSAGES_TABLE)
            .select("*")
            .eq("session_id", session_id)
            .order("created_at", desc=False)
            .limit(limit)
            .execute()
        )
        return result.data or []
    except Exception as e:
        print(f"âš ï¸ è·å–èŠå¤©å†å²å¤±è´¥: {e}")
        return []


@agent_chat_bp.route("/chat", methods=["POST"])
def chat():
    """æ™®é€šèŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()
        session_id = data.get("session_id") or str(uuid.uuid4())
        
        # ä»é…ç½®æ–‡ä»¶è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å†ä»å‰ç«¯ä¼ é€’ï¼‰
        temporary_prompts = data.get("temporary_prompts", [])  # å‰ç«¯å¯ä»¥ä¼ é€’ä¸´æ—¶æç¤ºè¯
        system_prompt = build_system_prompt(temporary_prompts=temporary_prompts)
        
        conversation_history = data.get("conversation_history", [])
        options = data.get("options", {})
        
        # åˆå¹¶é»˜è®¤é€‰é¡¹
        default_options = get_default_options()
        options = {**default_options, **options}
        
        if not user_message:
            return jsonify({"code": 400, "message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º", "data": None}), 400
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})
        
        # å…ˆåˆ›å»ºæˆ–æ›´æ–°ä¼šè¯ï¼ˆç¡®ä¿ä¼šè¯å­˜åœ¨ï¼‰
        _create_or_update_session(session_id)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        _save_message(session_id, "user", user_message)
        
        # è·å–ä»»åŠ¡ç±»å‹ï¼ˆä»å‰ç«¯ä¼ é€’æˆ–è‡ªåŠ¨æ£€æµ‹ï¼‰
        task_type = data.get("task_type", "auto")
        
        # æ ¹æ® task_type å†³å®šä½¿ç”¨å“ªä¸ªæœåŠ¡
        force_provider = None
        if task_type == 'research':
            force_provider = 'gpt-researcher'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ GPT-Researcherï¼ˆç ”ç©¶ä»»åŠ¡ï¼‰")
        elif task_type == 'data':
            force_provider = 'deepanalyze'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ DeepAnalyzeï¼ˆæ•°æ®åˆ†æä»»åŠ¡ï¼‰")
        elif task_type == 'chat':
            force_provider = 'qwen'
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ Qwenï¼ˆèŠå¤©ä»»åŠ¡ï¼‰")
        else:
            print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] ä½¿ç”¨è‡ªåŠ¨è·¯ç”±ï¼ˆtask_type: {task_type}ï¼‰")
        
        # è°ƒç”¨ LLM APIï¼ˆæ ¹æ® task_type é€‰æ‹©æœåŠ¡ï¼‰
        response = _call_llm_api(messages, user_message=user_message, stream=False, force_provider=force_provider, **options)
        result = response.json()
        
        # æå–AIå›å¤
        ai_content = ""
        if result.get("choices") and len(result["choices"]) > 0:
            ai_content = result["choices"][0].get("message", {}).get("content", "")
        elif result.get("output"):
            output = result["output"]
            if output.get("text"):
                ai_content = output["text"]
            elif output.get("choices") and len(output["choices"]) > 0:
                ai_content = output["choices"][0].get("message", {}).get("content", "") or output["choices"][0].get("text", "")
        
        if not ai_content:
            ai_content = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·æ¢ä¸ªæ–¹å¼æé—®ã€‚"
        
        # ä¿å­˜AIå›å¤
        _save_message(session_id, "assistant", ai_content)
        
        # æ›´æ–°ä¼šè¯
        _create_or_update_session(session_id)
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {
                "session_id": session_id,
                "content": ai_content,
                "conversation_history": conversation_history + [
                    {"role": "user", "content": user_message},
                    {"role": "assistant", "content": ai_content}
                ]
            }
        })
        
    except requests.exceptions.RequestException as e:
        error_msg = f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
        if hasattr(e, "response") and e.response is not None:
            try:
                error_data = e.response.json()
                error_msg = error_data.get("message", error_msg)
            except:
                pass
        return jsonify({"code": 500, "message": error_msg, "data": None}), 500
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/stream", methods=["POST"])
def chat_stream():
    """æµå¼èŠå¤©æ¥å£ï¼ˆSSEï¼‰"""
    try:
        data = request.get_json()
        user_message = data.get("message", "").strip()
        session_id = data.get("session_id") or str(uuid.uuid4())
        
        # ä»é…ç½®æ–‡ä»¶è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å†ä»å‰ç«¯ä¼ é€’ï¼‰
        temporary_prompts = data.get("temporary_prompts", [])  # å‰ç«¯å¯ä»¥ä¼ é€’ä¸´æ—¶æç¤ºè¯
        system_prompt = build_system_prompt(temporary_prompts=temporary_prompts)
        
        conversation_history = data.get("conversation_history", [])
        options = data.get("options", {})
        task_type = data.get("task_type", "auto")  # ä»»åŠ¡ç±»å‹ï¼š'research' å¼ºåˆ¶ä½¿ç”¨ GPT-Researcher, 'chat' ä½¿ç”¨ Qwen, 'auto' è‡ªåŠ¨è·¯ç”±
        
        # åˆå¹¶é»˜è®¤é€‰é¡¹
        default_options = get_default_options()
        options = {**default_options, **options}
        
        if not user_message:
            return jsonify({"code": 400, "message": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º", "data": None}), 400
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})
        
        # å…ˆåˆ›å»ºæˆ–æ›´æ–°ä¼šè¯ï¼ˆç¡®ä¿ä¼šè¯å­˜åœ¨ï¼‰
        _create_or_update_session(session_id)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_message_id = str(uuid.uuid4())
        _save_message(session_id, "user", user_message, user_message_id)
        
        def generate():
            """ç”Ÿæˆæµå¼å“åº”"""
            ai_content = ""
            ai_message_id = str(uuid.uuid4())
            
            try:
                # æ ¹æ® task_type å†³å®šä½¿ç”¨å“ªä¸ªæœåŠ¡
                use_gpt_researcher = False
                use_deepanalyze = False
                force_provider = None
                
                if task_type == 'research':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ GPT-Researcher
                    use_gpt_researcher = True
                    force_provider = 'gpt-researcher'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ GPT-Researcherï¼ˆç ”ç©¶ä»»åŠ¡ï¼‰")
                elif task_type == 'data':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ DeepAnalyze
                    use_deepanalyze = True
                    force_provider = 'deepanalyze'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ DeepAnalyzeï¼ˆæ•°æ®åˆ†æä»»åŠ¡ï¼‰")
                elif task_type == 'chat':
                    # å‰ç«¯æ˜ç¡®æŒ‡å®šä½¿ç”¨ Qwen
                    force_provider = 'qwen'
                    print(f"ğŸ¯ [ä»»åŠ¡æ§åˆ¶] å‰ç«¯æŒ‡å®šä½¿ç”¨ Qwenï¼ˆèŠå¤©ä»»åŠ¡ï¼‰")
                elif AUTO_ROUTE_TASKS and (USE_GPT_RESEARCHER or USE_DEEPANALYZE) and user_message:
                    # è‡ªåŠ¨è·¯ç”±
                    detected_task_type = detect_task_type(user_message)
                    if detected_task_type == 'research' and USE_GPT_RESEARCHER:
                        use_gpt_researcher = True
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°ç ”ç©¶ä»»åŠ¡ï¼Œè·¯ç”±åˆ° GPT-Researcher")
                        print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
                    elif detected_task_type == 'data' and USE_DEEPANALYZE:
                        use_deepanalyze = True
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®åˆ†æä»»åŠ¡ï¼Œè·¯ç”±åˆ° DeepAnalyze")
                        print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
                    else:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨æ£€æµ‹åˆ°{detected_task_type}ä»»åŠ¡ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡ (Qwen)")
                else:
                    if not AUTO_ROUTE_TASKS:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] è‡ªåŠ¨è·¯ç”±å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡")
                    elif not USE_GPT_RESEARCHER and not USE_DEEPANALYZE:
                        print(f"ğŸ” [ä»»åŠ¡è·¯ç”±] GPT-Researcher å’Œ DeepAnalyze å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤æœåŠ¡")
                
                # å‘é€åˆå§‹äº‹ä»¶
                yield f"data: {json.dumps({'type': 'start', 'session_id': session_id}, ensure_ascii=False)}\n\n"
                
                if use_gpt_researcher:
                    # ä½¿ç”¨ GPT-Researcherï¼ˆæ”¯æŒè¿›åº¦æ˜¾ç¤ºï¼‰
                    print(f"ğŸš€ [GPT-Researcher] å¼€å§‹è°ƒç”¨ç ”ç©¶æœåŠ¡...")
                    adapter = get_gpt_researcher_adapter()
                    
                    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆç”¨äºå‘é€è¿›åº¦åˆ°å‰ç«¯ï¼‰
                    progress_queue = []
                    def progress_callback(progress_data):
                        """å°†è¿›åº¦ä¿¡æ¯æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œç¨åé€šè¿‡ SSE å‘é€"""
                        progress_queue.append(progress_data)
                    
                    chunk_count = 0
                    for chunk in adapter.chat_completions_stream(
                        messages, 
                        progress_callback=progress_callback,
                        **options
                    ):
                        # å…ˆå‘é€æ‰€æœ‰ç´¯ç§¯çš„è¿›åº¦ä¿¡æ¯
                        while progress_queue:
                            progress_data = progress_queue.pop(0)
                            progress_json = json.dumps(progress_data, ensure_ascii=False)
                            yield f"data: {progress_json}\n\n"
                        
                        # å¤„ç†å†…å®¹å—
                        content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if content:
                            ai_content += content
                            chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                            yield f"data: {chunk_data}\n\n"
                            chunk_count += 1
                            # åªåœ¨å‰å‡ ä¸ªchunkæ‰“å°æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            if chunk_count <= 3:
                                print(f"ğŸ“¤ [GPT-Researcher] å‘é€chunk #{chunk_count}: {content[:30]}...")
                    
                    # å‘é€å‰©ä½™çš„è¿›åº¦ä¿¡æ¯
                    while progress_queue:
                        progress_data = progress_queue.pop(0)
                        progress_json = json.dumps(progress_data, ensure_ascii=False)
                        yield f"data: {progress_json}\n\n"
                    
                    print(f"âœ… [GPT-Researcher] å®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦: {len(ai_content)} å­—ç¬¦")
                elif use_deepanalyze:
                    # ä½¿ç”¨ DeepAnalyze APIï¼ˆåŸç”Ÿæµå¼æ”¯æŒï¼‰
                    print(f"ğŸš€ [DeepAnalyze] å¼€å§‹è°ƒç”¨æ•°æ®åˆ†ææœåŠ¡...")
                    adapter = get_deepanalyze_adapter()
                    
                    chunk_count = 0
                    for chunk in adapter.chat_completions_stream(messages, **options):
                        # å¤„ç†å†…å®¹å—
                        content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if content:
                            ai_content += content
                            chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                            yield f"data: {chunk_data}\n\n"
                            chunk_count += 1
                            # åªåœ¨å‰å‡ ä¸ªchunkæ‰“å°æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                            if chunk_count <= 3:
                                print(f"ğŸ“¤ [DeepAnalyze] å‘é€chunk #{chunk_count}: {content[:30]}...")
                    
                    print(f"âœ… [DeepAnalyze] å®Œæˆï¼Œå…±å‘é€ {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦: {len(ai_content)} å­—ç¬¦")
                else:
                    # ä½¿ç”¨ Qwen APIï¼ˆçœŸæ­£çš„æµå¼ï¼‰
                    response = _call_llm_api(messages, user_message=user_message, stream=True, force_provider=force_provider, **options)
                    
                    # å¤„ç†æµå¼å“åº”
                    for line in response.iter_lines():
                        if not line:
                            continue
                        
                        line_str = line.decode("utf-8")
                        
                        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                        if not line_str.strip() or line_str.startswith(':'):
                            continue
                        
                        # ç§»é™¤ "data: " å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if line_str.startswith("data: "):
                            line_str = line_str[6:]
                        
                        # æ£€æŸ¥ç»“æŸæ ‡è®°
                        if line_str.strip() == "[DONE]" or line_str.strip() == 'data:[DONE]':
                            break
                        
                        try:
                            chunk_data = json.loads(line_str)
                            
                            # å°è¯•å¤šç§æ ¼å¼è§£æ
                            content = ""
                            
                            # OpenAIå…¼å®¹æ ¼å¼
                            if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                                delta = chunk_data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                            
                            # DashScopeæ ¼å¼ï¼ˆQwenå¯èƒ½ä½¿ç”¨ï¼‰
                            elif "output" in chunk_data:
                                output = chunk_data["output"]
                                if "choices" in output and len(output["choices"]) > 0:
                                    choice = output["choices"][0]
                                    if "delta" in choice:
                                        content = choice["delta"].get("content", "")
                                    elif "message" in choice:
                                        content = choice["message"].get("content", "")
                                    elif "text" in choice:
                                        content = choice.get("text", "")
                                elif "text" in output:
                                    content = output.get("text", "")
                            
                            # ç›´æ¥æ–‡æœ¬æ ¼å¼
                            elif "text" in chunk_data:
                                content = chunk_data.get("text", "")
                            
                            # å¦‚æœæ‰¾åˆ°å†…å®¹ï¼Œç«‹å³å‘é€
                            if content:
                                ai_content += content
                                # ç«‹å³flushï¼Œç¡®ä¿å®æ—¶ä¼ è¾“
                                chunk_data = json.dumps({'type': 'chunk', 'content': content}, ensure_ascii=False)
                                yield f"data: {chunk_data}\n\n"
                                # è°ƒè¯•ï¼šæ‰“å°å‘é€çš„chunkï¼ˆä»…å‰å‡ ä¸ªå­—ç¬¦ï¼‰
                                if len(ai_content) <= 50:
                                    print(f"ğŸ“¤ [Qwen] å‘é€chunk: {content[:20]}...")
                                
                        except json.JSONDecodeError as e:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬ï¼Œè·³è¿‡
                            print(f"âš ï¸ è§£ææµå¼æ•°æ®å¤±è´¥: {e}, è¡Œå†…å®¹: {line_str[:100]}")
                            continue
                        except Exception as e:
                            print(f"âš ï¸ å¤„ç†æµå¼æ•°æ®å‡ºé”™: {e}")
                            continue
                
                # å‘é€å®Œæˆäº‹ä»¶
                yield f"data: {json.dumps({'type': 'done', 'session_id': session_id}, ensure_ascii=False)}\n\n"
                
                # ä¿å­˜å®Œæ•´çš„AIå›å¤
                if ai_content:
                    _save_message(session_id, "assistant", ai_content, ai_message_id)
                    _create_or_update_session(session_id)
                else:
                    # å¦‚æœæ²¡æœ‰æ”¶åˆ°å†…å®¹ï¼Œä¿å­˜é»˜è®¤æ¶ˆæ¯
                    default_msg = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·æ¢ä¸ªæ–¹å¼æé—®ã€‚"
                    _save_message(session_id, "assistant", default_msg, ai_message_id)
                    yield f"data: {json.dumps({'type': 'error', 'message': default_msg}, ensure_ascii=False)}\n\n"
                    
            except Exception as e:
                error_msg = f"æµå¼ä¼ è¾“é”™è¯¯: {str(e)}"
                yield f"data: {json.dumps({'type': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
        
        response = make_response(stream_with_context(generate()))
        response.headers["Content-Type"] = "text/event-stream; charset=utf-8"
        response.headers["Cache-Control"] = "no-cache, no-transform"
        response.headers["Connection"] = "keep-alive"
        response.headers["X-Accel-Buffering"] = "no"  # ç¦ç”¨Nginxç¼“å†²
        return response
        
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/history", methods=["GET"])
def get_chat_history():
    """è·å–èŠå¤©è®°å½•"""
    try:
        session_id = request.args.get("session_id")
        limit = int(request.args.get("limit", 50))
        
        if not session_id:
            return jsonify({"code": 400, "message": "session_idå‚æ•°å¿…å¡«", "data": None}), 400
        
        history = _get_chat_history(session_id, limit)
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        messages = []
        for msg in history:
            messages.append({
                "role": msg.get("role"),
                "content": msg.get("content"),
                "time": msg.get("created_at", ""),
            })
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {
                "session_id": session_id,
                "messages": messages
            }
        })
        
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/sessions", methods=["GET"])
def get_chat_sessions():
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯åˆ—è¡¨"""
    if not _supabase:
        return jsonify({"code": 200, "message": "success", "data": {"sessions": []}})
    
    try:
        limit = int(request.args.get("limit", 20))
        result = (
            _supabase.table(CHAT_SESSIONS_TABLE)
            .select("*")
            .order("updated_at", desc=True)
            .limit(limit)
            .execute()
        )
        
        sessions = result.data or []
        return jsonify({
            "code": 200,
            "message": "success",
            "data": {"sessions": sessions}
        })
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500


@agent_chat_bp.route("/chat/sessions/<session_id>", methods=["DELETE"])
def delete_chat_session(session_id):
    """åˆ é™¤èŠå¤©ä¼šè¯åŠå…¶æ‰€æœ‰æ¶ˆæ¯"""
    if not _supabase:
        return jsonify({"code": 200, "message": "success", "data": None})
    
    try:
        # åˆ é™¤æ¶ˆæ¯
        _supabase.table(CHAT_MESSAGES_TABLE).delete().eq("session_id", session_id).execute()
        # åˆ é™¤ä¼šè¯
        _supabase.table(CHAT_SESSIONS_TABLE).delete().eq("id", session_id).execute()
        
        return jsonify({
            "code": 200,
            "message": "success",
            "data": None
        })
    except Exception as e:
        return jsonify({"code": 500, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}", "data": None}), 500

