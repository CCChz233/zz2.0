# -*- coding: utf-8 -*-
"""
Êô∫ËÉΩ‰ΩìÂàùÂßãÊä•Âëä API Blueprint
----------------------------
Êé•Âè£ÔºöGET /api/agent/initial-report
ËØ¥ÊòéÔºö
    - ‰ºòÂÖà‰ªé Supabase ËßÜÂõæ/Ë°®ÊãâÂèñÊï∞ÊçÆ
    - Ë∂ÖÊó∂ÊàñÊï∞ÊçÆÁº∫Â§±Êó∂ÈôçÁ∫ß‰∏∫ÂÜÖÁΩÆÁ§∫‰æãÊï∞ÊçÆ
    - ËøîÂõûÁªìÊûÑ‰∏•Ê†ºÈÅµÂæ™ agent-report-api.md Á∫¶ÂÆö
"""

import json
import os
import threading
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

import requests

from flask import Blueprint, make_response

from infra import llm
from infra.db import supabase

# ===== ÈÖçÁΩÆ =====
AGENT_REPORT_SOURCE = os.getenv("AGENT_REPORT_SOURCE", "agent_initial_report_view")
AGENT_REPORT_LIMIT = int(os.getenv("AGENT_REPORT_LIMIT", "12"))
AGENT_REPORT_CACHE_TABLE = os.getenv("AGENT_REPORT_CACHE_TABLE", "agent_daily_report_cache")
AGENT_REPORT_SEARCH_DEPTH = os.getenv("AGENT_REPORT_SEARCH_DEPTH", "basic").strip() or "basic"
AGENT_REPORT_SEARCH_MAX_RESULTS = int(os.getenv("AGENT_REPORT_SEARCH_MAX_RESULTS", "6"))
AGENT_REPORT_SOURCE_LIMIT = int(os.getenv("AGENT_REPORT_SOURCE_LIMIT", "6"))
AGENT_REPORT_SUMMARY_MODEL = os.getenv("AGENT_REPORT_SUMMARY_MODEL") or None
AGENT_REPORT_REFRESH_MINUTES = int(os.getenv("AGENT_REPORT_REFRESH_MINUTES", "1440"))
AGENT_REPORT_GENERATION_ENABLED = (
    os.getenv("AGENT_REPORT_GENERATION_ENABLED", "true").strip().lower()
    in {"1", "true", "yes", "on"}
)

REPORT_CACHE_LOCK = threading.Lock()
REPORT_CACHE_STATE: Dict[str, Any] = {"generatedAt": None, "payload": None}

SECTION_CONFIGS = [
    {
        "id": 1,
        "title": "ÊîøÁ≠ñËß£ËØª",
        "heading": "ÊúÄÊñ∞ÊîøÁ≠ñÂä®ÊÄÅ",
        "icon": "el-icon-document-checked",
        "priority": 1,
        "query": "È´òÁ´ØÁßëÂ≠¶‰ª™Âô® ÂõΩ‰∫ßÂåñ ÊîøÁ≠ñ Âä®ÊÄÅ ËøëÊúü ËßÑÂàí Âª∫ËÆÆ",
    },
    {
        "id": 2,
        "title": "ËÆ∫ÊñáÊä•Âëä",
        "heading": "ÂâçÊ≤øÁ†îÁ©∂ËøõÂ±ï",
        "icon": "el-icon-reading",
        "priority": 2,
        "query": "ÂéüÂ≠êÂäõÊòæÂæÆÈïú Á£ÅÊÄßÊé¢Èíà Á∫≥Á±≥Ë°®ÂæÅ ÊúÄÊñ∞Á†îÁ©∂ ËÆ∫Êñá ËøõÂ±ï",
    },
    {
        "id": 3,
        "title": "Â∏ÇÂú∫Âä®ÊÄÅ",
        "heading": "ÊúÄÊñ∞‰∫ß‰∏öÂä®ÊÄÅ",
        "icon": "el-icon-data-line",
        "priority": 3,
        "query": "Á≤æÂØÜ‰ª™Âô® Ë°å‰∏ö Âä®ÊÄÅ ‰∫ß‰∏ö Âêà‰Ωú ÊäïËµÑ ËøëÊúü",
    },
]

agent_report_bp = Blueprint("agent_report", __name__)
_supabase = supabase

# ===== ÂõûÈÄÄÊï∞ÊçÆÔºà‰∏éÊñáÊ°£Á§∫‰æã‰∏ÄËá¥Ôºâ =====
_FALLBACK_GENERATED_AT = "2023-11-15T10:30:00Z"
_FALLBACK_SECTIONS: List[Dict[str, Any]] = [
    {
        "id": 1,
        "title": "ÊîøÁ≠ñËß£ËØª",
        "icon": "el-icon-document-checked",
        "content": (
            "## ÊúÄÊñ∞ÊîøÁ≠ñÂä®ÊÄÅ\n\n"
            "**‰∏≠Â§ÆÂèëÂ∏É\"ÂçÅ‰∫î‰∫î\"ËßÑÂàíÂª∫ËÆÆÔºåÊòéÁ°Æ‰∫ß‰∏ö‰∏éÁßëÊäÄÂèëÂ±ïÈáçÁÇπ**\n\n"
            "ËøëÊó•ÔºåÂÖöÁöÑ‰∫åÂçÅÂ±äÂõõ‰∏≠ÂÖ®‰ºöÂÆ°ËÆÆÈÄöËøá‰∫Ü„Ää‰∏≠ÂÖ±‰∏≠Â§ÆÂÖ≥‰∫éÂà∂ÂÆöÂõΩÊ∞ëÁªèÊµéÂíåÁ§æ‰ºöÂèëÂ±ïÁ¨¨ÂçÅ‰∫î‰∏™‰∫îÂπ¥ËßÑÂàíÁöÑÂª∫ËÆÆ„ÄãÔºåÊòéÁ°ÆÊèêÂá∫Ôºö\n\n"
            "* Êé®Âä®**Âª∫ËÆæÁé∞‰ª£Âåñ‰∫ß‰∏ö‰ΩìÁ≥ª**ÔºåÂ∑©Âõ∫Â£ÆÂ§ßÂÆû‰ΩìÁªèÊµéÊ†πÂü∫\n"
            "* Âä†Âø´ÂÆûÁé∞**È´òÊ∞¥Âπ≥ÁßëÊäÄËá™Á´ãËá™Âº∫**ÔºåÁ™ÅÁ†¥ÂÖ≥ÈîÆÊ†∏ÂøÉÊäÄÊúØ\n"
            "* ÂÖ®Èù¢Êé®Ëøõ**Êï∞Â≠ó‰∏≠ÂõΩÂª∫ËÆæ**ÔºåÂÆûÊñΩ\"‰∫∫Â∑•Êô∫ËÉΩ+\"Ë°åÂä®\n\n"
            "### Ê†∏ÂøÉË¶ÅÁÇπ\n\n"
            "1. **È´òÁ´ØÊäÄÊúØÊñπÂêë**ÔºöËÅöÁÑ¶**ÈõÜÊàêÁîµË∑Ø„ÄÅÂ∑•‰∏öÊØçÊú∫„ÄÅÈ´òÁ´Ø‰ª™Âô®**Á≠âÈ¢ÜÂüüÂºÄÂ±ïÊîªÂÖ≥\n"
            "2. **ÁßëÊäÄ‰∏é‰∫ß‰∏öËûçÂêà**ÔºöÊîØÊåÅ‰ºÅ‰∏öÁâµÂ§¥ËÅîÂêàÊîªÂÖ≥ÔºåÂä†Âº∫ÊàêÊûúËΩ¨ÂåñÂ∫îÁî®\n"
            "3. **Êï∞Â≠óÂü∫Á°ÄËÉΩÂäõ**ÔºöÂº∫Âåñ**ÁÆóÂäõ„ÄÅÁÆóÊ≥ï„ÄÅÊï∞ÊçÆ**Á≠âÈ´òÊïà‰æõÁªôÔºåÂÖ®Êñπ‰ΩçËµãËÉΩÂçÉË°åÁôæ‰∏ö\n\n"
            "> üìå \"ÂçÅ‰∫î‰∫î\"ËßÑÂàíÂª∫ËÆÆÂØπ‰ª™Âô®Ë£ÖÂ§á„ÄÅËá™‰∏ªÁ†îÂèëËÉΩÂäõ‰∏éÊô∫ËÉΩÊäÄÊúØÊèêÂá∫Á≥ªÁªüÈÉ®ÁΩ≤ÔºåÈáäÊîæÊòéÁ°ÆÊîøÁ≠ñÂØºÂêë„ÄÇ"
        ),
        "priority": 1,
        "updatedAt": "2023-11-15T09:00:00Z",
    },
    {
        "id": 2,
        "title": "ËÆ∫ÊñáÊä•Âëä",
        "icon": "el-icon-reading",
        "content": (
            "## ÂâçÊ≤øÁ†îÁ©∂ËøõÂ±ï\n\n"
            "### Nanoscale ÊúÄÊñ∞ÂèëË°®\n\n"
            "**Temperature-dependent sign reversal of tunneling magnetoresistance in van der Waals ferromagnetic heterojunctions**\n\n"
            "Ë•øÂÆâ‰∫§ÈÄöÂ§ßÂ≠¶ÊùêÊñôÁßëÂ≠¶‰∏éÂ∑•Á®ãÂ≠¶Èô¢Ëá™ÊóãÁîµÂ≠ê‰∏éÈáèÂ≠êÁ≥ªÁªüÁ†îÁ©∂‰∏≠ÂøÉÂõ¢Èòü‰∫é„ÄäNanoscale„ÄãÊúüÂàäÂèëË°®Á†îÁ©∂ÊàêÊûúÔºåÊè≠Á§∫Á£ÅÈößÈÅìÁªì‰∏≠TMR‰ø°Âè∑ÈöèÊ∏©Â∫¶ÂèòÂåñÂèëÁîüÊ≠£Ë¥üÂèçËΩ¨ÁöÑÁâ©ÁêÜÊú∫Âà∂Ôºö\n\n"
            "* ÊûÑÂª∫ CrVI‚ÇÜ / Fe‚ÇÉGeTe‚ÇÇ ÂºÇË¥®ÁªìÊûÑÔºåËßÇÂØüÂà∞Â±ÖÈáåÊ∏©Â∫¶ÈôÑËøëÂá∫Áé∞**TMRÁ¨¶Âè∑ÂèçËΩ¨**\n"
            "* ÂÆûÈ™åÈ™åËØÅ**ÂèçÈìÅÁ£ÅËÄ¶Âêà**ÊòØTMRÂèçËΩ¨ÁöÑÊ†∏ÂøÉÊú∫Âà∂\n"
            "* ÂèëÁé∞**Ê∏©Â∫¶+ÂÅèÂéãËÅîÂä®Ë∞ÉÊéß‰∏ãÂèØÂÆûÁé∞Â§öÊÄÅTMRË°å‰∏∫**\n\n"
            "### ÂÆûÈ™åÊîØÊíëËÆæÂ§á\n\n"
            "ËØ•Á†îÁ©∂‰æùÊâò**Ëá¥ÁúüÁ≤æÂØÜ‰ª™Âô® KMP-L Á≥ªÁªü**ÂÆåÊàêÂÖ≥ÈîÆÊµãËØïÔºö\n\n"
            "* ÊàêÂäüÂÆûÁé∞**‰ΩéÊ∏©Âº∫Âú∫ÂæÆÂå∫Á£ÅÁï¥ÊàêÂÉè**\n"
            "* Âú®50K‰∏ãËßÇÂØüÂºÇË¥®Â±ÇÂëàÁé∞**Áõ∏ÂèçÁ£ÅË°¨Â∫¶**\n"
            "* Á≥ªÁªüÊèê‰æõÁ©∫Èó¥ÂàÜËæ®ÁöÑMOKEÊµãÈáèÔºåÁõ¥Êé•ËØÅÂÆûAFMËÄ¶ÂêàÂ≠òÂú®\n\n"
            "> üí° KMP-LÁ≥ªÁªüÊàê‰∏∫ËøûÊé•ÊùêÊñôÂæÆËßÇÁ£ÅÁªìÊûÑ‰∏éÂô®‰ª∂ÂÆèËßÇÊÄßËÉΩÁöÑÈáçË¶ÅÁ∫ΩÂ∏¶ÔºåÊòæËëóÊîØÊíë‰∫ÜËØ•Á†îÁ©∂ÊàêÊûúÁöÑÈ™åËØÅËøáÁ®ã„ÄÇ"
        ),
        "priority": 2,
        "updatedAt": "2023-11-15T08:30:00Z",
    },
    {
        "id": 3,
        "title": "Â∏ÇÂú∫Âä®ÊÄÅ",
        "icon": "el-icon-data-line",
        "content": (
            "## ÊúÄÊñ∞‰∫ß‰∏öÂä®ÊÄÅ\n\n"
            "**ÂØåÁùøÊÄù√óÂ§©ÁéëÁÆóÂÖ±Âª∫ÂéüÂ≠êÂäõÊòæÂæÆÈïúÂàÜÊûêÊµãËØï‰∏≠ÂøÉÔºåËêΩÂú∞ÊàêÈÉΩ**\n\n"
            "2025Âπ¥9Êúà25Êó•ÔºåÂØåÁùøÊÄù‰∏éÂ§©ÁéëÁÆóÂú®ÊàêÈÉΩÁ≠æÁΩ≤Âêà‰ΩúÂçèËÆÆÔºåËÅîÂêàËÆæÁ´ã**ÂéüÂ≠êÂäõÊòæÂæÆÈïúÔºàAFMÔºâÂàÜÊûêÊµãËØï‰∏≠ÂøÉ**ÔºåÊó®Âú®Êé®Âä®È´òÁ´ØÁ≤æÂØÜÊ£ÄÊµãËµÑÊ∫êÊõ¥Â•ΩÊúçÂä°ÁßëÁ†î‰∏é‰∫ß‰∏ö‰∏ÄÁ∫ø„ÄÇ\n\n"
            "### Ê†∏ÂøÉË¶ÅÁÇπ\n\n"
            "1. **Âêà‰ΩúÂÜÖÂÆπ**Ôºö‰∏≠ÂøÉËÅöÁÑ¶**ÂΩ¢Ë≤åË°®ÂæÅ„ÄÅÁâ©ÊÄßÂàÜÊûê„ÄÅÂ∑•‰∏öÁ∫ßÊ£ÄÊµãÊ†°ÂáÜ**Á≠âÊ†∏ÂøÉÊñπÂêë\n"
            "2. **ËÆæÂ§áÊîØÊíë**ÔºöÂØåÁùøÊÄùÊèê‰æõ**ÁßëÁ†îËá≥ËÆ°ÈáèÁ∫ß**ÂÖ®Á≥ªÂàóAFM‰∫ßÂìÅÔºåÊäÄÊúØÊ∂µÁõñ\"True3DÊâ´Êèè\"\"Ëá™Âä®Êç¢ÈíàÁ≥ªÁªü\"Á≠â\n"
            "3. **ÊúçÂä°‰ΩìÁ≥ª**ÔºöÂ§©ÁéëÁÆóÊèê‰æõ**ÂÆûÈ™åÊ£ÄÊµã„ÄÅÁÆóÂäõÊîØÊåÅ‰∏éÂÆöÂà∂ÂåñÁßëÁ†îÊúçÂä°**ÔºåÂΩ¢Êàê\"ËÆæÂ§á+ÊúçÂä°\"‰∏Ä‰ΩìÂåñËß£ÂÜ≥ÊñπÊ°à\n\n"
            "> üìå Ê≠§Ê¨°Âêà‰Ωú‰∏∫È´òÁ´ØÊ£ÄÊµãËÆæÂ§áÂú®ÁßëÁ†î‰∏éÂ∑•Á®ãÈ¢ÜÂüüÁöÑÊ∑±ÂÖ•Â∫îÁî®Êê≠Âª∫Êñ∞Âπ≥Âè∞ÔºåÂä©ÂäõËµÑÊ∫êÊï¥Âêà‰∏éËÉΩÂäõÊèêÂçá„ÄÇ"
        ),
        "priority": 3,
        "updatedAt": "2023-11-15T07:00:00Z",
    },
]


def _to_iso(value: Optional[Any]) -> Optional[str]:
    """Â∞Ü str/datetime ËΩ¨Êàê ISO8601 UTCÔºàÊó†ÂæÆÁßíÔºâÔºõÂÖ∂‰ΩôÁ±ªÂûãËøîÂõû None„ÄÇ"""
    if value is None:
        return None
    if isinstance(value, datetime):
        dt = value.astimezone(timezone.utc) if value.tzinfo else value.replace(tzinfo=timezone.utc)
        return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
    if isinstance(value, str):
        txt = value.strip()
        if not txt:
            return None
        try:
            if txt.endswith("Z"):
                txt = txt.replace("Z", "+00:00")
            dt = datetime.fromisoformat(txt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            else:
                dt = dt.astimezone(timezone.utc)
            return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
        except Exception:
            return value
    return None


def _parse_datetime(value: Optional[Any]) -> Optional[datetime]:
    if value is None:
        return None
    if isinstance(value, datetime):
        return value.astimezone(timezone.utc) if value.tzinfo else value.replace(tzinfo=timezone.utc)
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return None
        try:
            if text.endswith("Z"):
                text = text.replace("Z", "+00:00")
            dt = datetime.fromisoformat(text)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            else:
                dt = dt.astimezone(timezone.utc)
            return dt
        except Exception:
            return None
    return None


def _is_fresh(generated_at: Optional[str], now: Optional[datetime] = None) -> bool:
    if AGENT_REPORT_REFRESH_MINUTES <= 0:
        return False
    dt = _parse_datetime(generated_at)
    if not dt:
        return False
    now = now or datetime.now(timezone.utc)
    return now - dt <= timedelta(minutes=AGENT_REPORT_REFRESH_MINUTES)


def _get_tavily_key() -> str:
    return os.getenv("TAVILY_API_KEY", "").strip()


def _today_key() -> str:
    return datetime.now().strftime("%Y-%m-%d")


def _fetch_cached_report(date_key: str) -> Optional[Tuple[str, List[Dict[str, Any]]]]:
    if not _supabase:
        return None
    try:
        res = (
            _supabase.table(AGENT_REPORT_CACHE_TABLE)
            .select("generated_at, sections, source, updated_at")
            .eq("cache_date", date_key)
            .order("updated_at", desc=True)
            .limit(1)
            .execute()
        )
        rows = res.data or []
        if not rows:
            return None
        row = rows[0]
        sections = row.get("sections")
        if not isinstance(sections, list):
            return None
        generated_at = row.get("generated_at") or row.get("updated_at") or _FALLBACK_GENERATED_AT
        return generated_at, sections
    except Exception as exc:
        print(f"‚ö†Ô∏è ËØªÂèñÊó•Êä•ÁºìÂ≠òÂ§±Ë¥•: {exc}")
        return None

def _fetch_latest_cached_report() -> Optional[Tuple[str, List[Dict[str, Any]]]]:
    if not _supabase:
        return None
    try:
        res = (
            _supabase.table(AGENT_REPORT_CACHE_TABLE)
            .select("cache_date, generated_at, sections, source, updated_at, created_at")
            .order("generated_at", desc=True)
            .order("updated_at", desc=True)
            .limit(1)
            .execute()
        )
        rows = res.data or []
        if not rows:
            return None
        row = rows[0]
        sections = row.get("sections")
        if not isinstance(sections, list):
            return None
        generated_at = (
            row.get("generated_at")
            or row.get("updated_at")
            or row.get("created_at")
            or _FALLBACK_GENERATED_AT
        )
        return generated_at, sections
    except Exception as exc:
        print(f"‚ö†Ô∏è ËØªÂèñÊúÄÊñ∞Êó•Êä•ÁºìÂ≠òÂ§±Ë¥•: {exc}")
        return None


def _save_cached_report(
    date_key: str,
    generated_at: str,
    sections: List[Dict[str, Any]],
    source: str,
) -> None:
    if not _supabase:
        return
    payload = {
        "cache_date": date_key,
        "generated_at": generated_at,
        "sections": sections,
        "source": source,
        "updated_at": _to_iso(datetime.now(timezone.utc)) or _FALLBACK_GENERATED_AT,
    }
    try:
        _supabase.table(AGENT_REPORT_CACHE_TABLE).insert(payload).execute()
    except Exception as exc:
        msg = str(exc)
        if "duplicate key value" in msg or "unique constraint" in msg:
            print("‚ö†Ô∏è ÂÜôÂÖ•Êó•Êä•ÁºìÂ≠òÂ§±Ë¥•ÔºöÁºìÂ≠òË°®‰ªçÊòØÂçïÊó•ÂîØ‰∏ÄÈîÆÔºåËØ∑ÂÖàÂçáÁ∫ßË°®ÁªìÊûÑ‰ª•ÊîØÊåÅÂêåÊó•Â§öÊù°ËÆ∞ÂΩï")
        else:
            print(f"‚ö†Ô∏è ÂÜôÂÖ•Êó•Êä•ÁºìÂ≠òÂ§±Ë¥•: {exc}")


def _normalize_date(value: Optional[Any]) -> Optional[str]:
    if not value:
        return None
    text = str(value).strip()
    if not text:
        return None
    try:
        if text.endswith("Z"):
            text = text.replace("Z", "+00:00")
        dt = datetime.fromisoformat(text)
        return dt.date().isoformat()
    except Exception:
        return text[:10] if len(text) >= 10 else None


def _extract_source_name(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        return parsed.netloc or ""
    except Exception:
        return ""


def _truncate(text: str, limit: int) -> str:
    if not text:
        return ""
    cleaned = " ".join(text.split())
    if len(cleaned) <= limit:
        return cleaned
    return cleaned[:limit].rstrip() + "..."


def _normalize_tavily_result(raw: dict) -> dict:
    title = str(raw.get("title") or "").strip()
    url = str(raw.get("url") or "").strip()
    snippet = (
        str(raw.get("content") or raw.get("snippet") or raw.get("summary") or "").strip()
    )
    published_at = _normalize_date(
        raw.get("published_date")
        or raw.get("published_time")
        or raw.get("published")
        or raw.get("date")
    )
    source = str(raw.get("source") or "").strip() or _extract_source_name(url)
    return {
        "title": title,
        "url": url,
        "snippet": _truncate(snippet, 260),
        "publishedAt": published_at,
        "source": source,
    }


def _dedupe_results(results: List[dict]) -> List[dict]:
    seen = set()
    deduped = []
    for item in results:
        key = item.get("url") or item.get("title")
        if not key or key in seen:
            continue
        seen.add(key)
        deduped.append(item)
    return deduped


def _call_tavily_search(query: str, max_results: int) -> List[dict]:
    api_key = _get_tavily_key()
    if not api_key:
        raise RuntimeError("missing TAVILY_API_KEY")
    payload = {
        "api_key": api_key,
        "query": query,
        "search_depth": AGENT_REPORT_SEARCH_DEPTH,
        "max_results": max_results,
        "include_answer": False,
        "include_images": False,
        "include_raw_content": False,
    }
    timeout = int(os.getenv("TAVILY_TIMEOUT", "25"))
    try:
        response = requests.post("https://api.tavily.com/search", json=payload, timeout=timeout)
        response.raise_for_status()
        data = response.json() or {}
    except requests.RequestException as exc:
        detail = ""
        if getattr(exc, "response", None) is not None:
            try:
                detail = exc.response.text[:300]
            except Exception:
                detail = ""
        print(f"‚ö†Ô∏è Tavily ËØ∑Ê±ÇÂ§±Ë¥•: {exc} {detail}".strip())
        raise
    raw_results = data.get("results") or []
    normalized = [_normalize_tavily_result(item) for item in raw_results if isinstance(item, dict)]
    normalized = [item for item in normalized if item.get("url") and item.get("title")]
    return _dedupe_results(normalized)


def _llm_summarize_section(heading: str, results: List[dict]) -> str:
    if not results:
        return "- ÊöÇÊó†ÂèØÈù†ÂÖ¨ÂºÄ‰ø°ÊÅØÔºåÂèØÁ®çÂêéÂà∑Êñ∞Êü•Áúã"

    context_lines = []
    for idx, item in enumerate(results[:AGENT_REPORT_SOURCE_LIMIT], start=1):
        title = item.get("title") or "Êú™ÂëΩÂêç"
        snippet = item.get("snippet") or ""
        source = item.get("source") or "Êú™Áü•Êù•Ê∫ê"
        published = item.get("publishedAt") or "Êú™Áü•Êó•Êúü"
        url = item.get("url") or ""
        context_lines.append(
            f"{idx}. Ê†áÈ¢òÔºö{title}\nÊëòË¶ÅÔºö{snippet}\nÊù•Ê∫êÔºö{source}\nÂèëÂ∏ÉÊó∂Èó¥Ôºö{published}\nÈìæÊé•Ôºö{url}"
        )

    prompt = (
        f"ËØ∑Ê†πÊçÆ‰∏ãÈù¢ÁöÑÊ£ÄÁ¥¢ÁªìÊûúÔºåÁîüÊàê„Ää{heading}„ÄãÁÆÄÊä•„ÄÇ\n"
        "Ë¶ÅÊ±ÇÔºö\n"
        "1) Áî®‰∏≠ÊñáËæìÂá∫Ôºõ\n"
        "2) 3-5Êù°Ë¶ÅÁÇπÔºå‰ΩøÁî®È°πÁõÆÁ¨¶Âè∑Ôºõ\n"
        "3) ÊÄªÂ≠óÊï∞ÊéßÂà∂Âú®120-220Â≠óÔºõ\n"
        "4) ‰∏çË¶ÅÊçèÈÄ†‰∫ãÂÆûÔºå‰∏çË¶ÅÁºñÈÄ†Êù•Ê∫êÔºõ\n"
        "5) ‰∏çË¶ÅÂåÖÂê´ÈìæÊé•ÊàñÂºïÁî®Ê†ºÂºè„ÄÇ\n\n"
        "Ê£ÄÁ¥¢ÁªìÊûúÔºö\n"
        + "\n\n".join(context_lines)
    )

    messages = [
        {
            "role": "system",
            "content": "‰Ω†ÊòØ‰ºÅ‰∏öÊÉÖÊä•Âä©ÊâãÔºåËæìÂá∫Á≤æÁÇº„ÄÅÁªìÊûÑÂåñÁöÑ‰∏≠ÊñáË¶ÅÁÇπ„ÄÇ",
        },
        {"role": "user", "content": prompt},
    ]

    try:
        resp = llm.chat(
            messages,
            temperature=0.3,
            top_p=0.8,
            max_tokens=500,
            model=AGENT_REPORT_SUMMARY_MODEL,
            timeout=60,
        )
        data = resp.json() if hasattr(resp, "json") else {}
        content = (
            data.get("choices", [{}])[0]
            .get("message", {})
            .get("content", "")
            .strip()
        )
        if content:
            return content
    except Exception:
        pass

    fallback_lines = [f"- {item.get('title')}" for item in results[:3] if item.get("title")]
    return "\n".join(fallback_lines) or "- ÊöÇÊó†ÂèØÈù†ÂÖ¨ÂºÄ‰ø°ÊÅØÔºåÂèØÁ®çÂêéÂà∑Êñ∞Êü•Áúã"


def _build_dynamic_sections() -> Tuple[str, List[Dict[str, Any]]]:
    generated_at = _to_iso(datetime.now(timezone.utc)) or _FALLBACK_GENERATED_AT
    sections: List[Dict[str, Any]] = []

    for section in SECTION_CONFIGS:
        try:
            results = _call_tavily_search(
                section["query"],
                max_results=AGENT_REPORT_SEARCH_MAX_RESULTS,
            )
        except Exception as exc:
            print(f"‚ö†Ô∏è Tavily ÊêúÁ¥¢Â§±Ë¥• [{section['title']}]: {exc}")
            results = []
        summary = _llm_summarize_section(section["heading"], results)
        content = f"## {section['heading']}\n\n{summary}"

        sources = []
        for item in results[:AGENT_REPORT_SOURCE_LIMIT]:
            sources.append(
                {
                    "title": item.get("title"),
                    "url": item.get("url"),
                    "source": item.get("source"),
                    "publishedAt": item.get("publishedAt"),
                }
            )

        sections.append(
            {
                "id": section["id"],
                "title": section["title"],
                "icon": section["icon"],
                "content": content,
                "priority": section["priority"],
                "updatedAt": generated_at,
                "sources": sources,
            }
        )

    sections.sort(key=lambda s: (s.get("priority", 99), s.get("id", 0)))
    return generated_at, sections


def _get_cached_or_refresh(force_refresh: bool, can_generate: bool) -> Tuple[str, List[Dict[str, Any]], str]:
    now = datetime.now(timezone.utc)

    if not force_refresh:
        with REPORT_CACHE_LOCK:
            cached_payload = REPORT_CACHE_STATE.get("payload")
            cached_generated = REPORT_CACHE_STATE.get("generatedAt")
        if cached_payload and _is_fresh(cached_generated, now):
            return cached_payload["generatedAt"], cached_payload["sections"], "cache"

    cached_latest = _fetch_latest_cached_report()
    if cached_latest:
        generated_at, sections = cached_latest
        payload = {"generatedAt": generated_at, "sections": sections}
        with REPORT_CACHE_LOCK:
            REPORT_CACHE_STATE["generatedAt"] = generated_at
            REPORT_CACHE_STATE["payload"] = payload
        if not force_refresh and _is_fresh(generated_at, now):
            return generated_at, sections, "cache-db"
        if not can_generate:
            return generated_at, sections, "cache-stale"
    elif not can_generate:
        raise RuntimeError("report generation disabled and no cache available")

    generated_at, sections = _build_dynamic_sections()
    payload = {"generatedAt": generated_at, "sections": sections}
    with REPORT_CACHE_LOCK:
        REPORT_CACHE_STATE["generatedAt"] = generated_at
        REPORT_CACHE_STATE["payload"] = payload

    cache_date = _normalize_date(generated_at) or _today_key()
    _save_cached_report(cache_date, generated_at, sections, "tavily")
    return generated_at, sections, "tavily"


def _fetch_from_supabase(limit: int) -> Tuple[str, List[Dict[str, Any]]]:
    if not _supabase:
        raise RuntimeError("Supabase client not available")

    query = (
        _supabase.table(AGENT_REPORT_SOURCE)
        .select("*")
        .order("priority", desc=False)
        .order("updated_at", desc=True)
    )
    if limit > 0:
        query = query.limit(limit)
    res = query.execute()
    rows = res.data or []

    if not rows:
        raise ValueError("no rows in Supabase result")

    sections: List[Dict[str, Any]] = []
    generated_candidates: List[str] = []

    for row in rows:
        section_id = row.get("id")
        title = row.get("title")
        content = row.get("content")
        icon = row.get("icon")

        if section_id is None or not title or not content:
            continue

        priority_val = row.get("priority")
        try:
            priority_int = int(priority_val)
        except Exception:
            priority_int = 99

        updated_at = row.get("updated_at") or row.get("updatedAt")
        formatted_updated = _to_iso(updated_at) or _to_iso(row.get("created_at"))

        sections.append(
            {
                "id": int(section_id),
                "title": str(title),
                "icon": str(icon) if icon else "el-icon-reading",
                "content": str(content),
                "priority": priority_int,
                "updatedAt": formatted_updated or _FALLBACK_GENERATED_AT,
                "sources": row.get("sources") if isinstance(row.get("sources"), list) else [],
            }
        )

        generated_raw = row.get("generated_at") or row.get("generatedAt")
        formatted_generated = _to_iso(generated_raw)
        if formatted_generated:
            generated_candidates.append(formatted_generated)

    if not sections:
        raise ValueError("no valid sections from Supabase")

    sections.sort(key=lambda s: (s["priority"], s["id"]))

    generated_at = (
        generated_candidates[0]
        if generated_candidates
        else sections[0].get("updatedAt")
        or _FALLBACK_GENERATED_AT
    )

    return generated_at, sections


def _fallback_report() -> Tuple[str, List[Dict[str, Any]]]:
    return _FALLBACK_GENERATED_AT, _FALLBACK_SECTIONS


@agent_report_bp.route("/initial-report", methods=["GET"])
def get_agent_initial_report():
    force_refresh = False
    try:
        from flask import request

        force_refresh = request.args.get("refresh") == "1"
    except Exception:
        force_refresh = False

    tavily_key = _get_tavily_key()
    if not tavily_key:
        print("[WARN] TAVILY_API_KEY missing; Tavily fetch disabled")

    can_generate = bool(tavily_key) and (AGENT_REPORT_GENERATION_ENABLED or force_refresh)
    try:
        generated_at, sections, source = _get_cached_or_refresh(force_refresh, can_generate)
    except Exception:
        try:
            generated_at, sections = _fetch_from_supabase(AGENT_REPORT_LIMIT)
            source = "remote"
        except Exception:
            generated_at, sections = _fallback_report()
            source = "fallback"

    payload = {
        "code": 200,
        "message": "success",
        "data": {
            "generatedAt": generated_at,
            "sections": sections,
        },
        "source": source,
    }

    try:
        print(f"[INFO] agent initial report source={source} generatedAt={generated_at}")
    except Exception:
        pass

    response = make_response(json.dumps(payload, ensure_ascii=False, indent=2))
    response.status_code = 200
    response.mimetype = "application/json; charset=utf-8"
    return response


# Ê≠§ Blueprint ‰ªÖ‰æõ app.py Ê≥®ÂÜå‰ΩøÁî®ÔºåÊó†ÈúÄÁã¨Á´ãËøêË°åÂÖ•Âè£
