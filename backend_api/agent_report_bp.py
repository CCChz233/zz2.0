# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä½“åˆå§‹æŠ¥å‘Š API Blueprint
----------------------------
æ¥å£ï¼šGET /api/agent/initial-report
è¯´æ˜ï¼š
    - ä¼˜å…ˆä» Supabase è§†å›¾/è¡¨æ‹‰å–æ•°æ®
    - è¶…æ—¶æˆ–æ•°æ®ç¼ºå¤±æ—¶é™çº§ä¸ºå†…ç½®ç¤ºä¾‹æ•°æ®
    - è¿”å›ç»“æ„ä¸¥æ ¼éµå¾ª agent-report-api.md çº¦å®š
"""

import json
import os
import threading
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

import requests

from flask import Blueprint, make_response

from infra import llm
from infra.db import supabase

# ===== é…ç½® =====
AGENT_REPORT_SOURCE = os.getenv("AGENT_REPORT_SOURCE", "agent_initial_report_view")
AGENT_REPORT_LIMIT = int(os.getenv("AGENT_REPORT_LIMIT", "12"))
AGENT_REPORT_CACHE_TABLE = os.getenv("AGENT_REPORT_CACHE_TABLE", "agent_daily_report_cache")
AGENT_REPORT_SEARCH_DEPTH = os.getenv("AGENT_REPORT_SEARCH_DEPTH", "basic").strip() or "basic"
AGENT_REPORT_SEARCH_MAX_RESULTS = int(os.getenv("AGENT_REPORT_SEARCH_MAX_RESULTS", "6"))
AGENT_REPORT_SOURCE_LIMIT = int(os.getenv("AGENT_REPORT_SOURCE_LIMIT", "6"))
AGENT_REPORT_SUMMARY_MODEL = os.getenv("AGENT_REPORT_SUMMARY_MODEL") or None

REPORT_CACHE_LOCK = threading.Lock()
REPORT_CACHE_STATE: Dict[str, Any] = {"date": None, "payload": None}

SECTION_CONFIGS = [
    {
        "id": 1,
        "title": "æ”¿ç­–è§£è¯»",
        "heading": "æœ€æ–°æ”¿ç­–åŠ¨æ€",
        "icon": "el-icon-document-checked",
        "priority": 1,
        "query": "é«˜ç«¯ç§‘å­¦ä»ªå™¨ å›½äº§åŒ– æ”¿ç­– åŠ¨æ€ è¿‘æœŸ è§„åˆ’ å»ºè®®",
    },
    {
        "id": 2,
        "title": "è®ºæ–‡æŠ¥å‘Š",
        "heading": "å‰æ²¿ç ”ç©¶è¿›å±•",
        "icon": "el-icon-reading",
        "priority": 2,
        "query": "åŸå­åŠ›æ˜¾å¾®é•œ ç£æ€§æ¢é’ˆ çº³ç±³è¡¨å¾ æœ€æ–°ç ”ç©¶ è®ºæ–‡ è¿›å±•",
    },
    {
        "id": 3,
        "title": "å¸‚åœºåŠ¨æ€",
        "heading": "æœ€æ–°äº§ä¸šåŠ¨æ€",
        "icon": "el-icon-data-line",
        "priority": 3,
        "query": "ç²¾å¯†ä»ªå™¨ è¡Œä¸š åŠ¨æ€ äº§ä¸š åˆä½œ æŠ•èµ„ è¿‘æœŸ",
    },
]

agent_report_bp = Blueprint("agent_report", __name__)
_supabase = supabase

# ===== å›é€€æ•°æ®ï¼ˆä¸æ–‡æ¡£ç¤ºä¾‹ä¸€è‡´ï¼‰ =====
_FALLBACK_GENERATED_AT = "2023-11-15T10:30:00Z"
_FALLBACK_SECTIONS: List[Dict[str, Any]] = [
    {
        "id": 1,
        "title": "æ”¿ç­–è§£è¯»",
        "icon": "el-icon-document-checked",
        "content": (
            "## æœ€æ–°æ”¿ç­–åŠ¨æ€\n\n"
            "**ä¸­å¤®å‘å¸ƒ\"åäº”äº”\"è§„åˆ’å»ºè®®ï¼Œæ˜ç¡®äº§ä¸šä¸ç§‘æŠ€å‘å±•é‡ç‚¹**\n\n"
            "è¿‘æ—¥ï¼Œå…šçš„äºŒåå±Šå››ä¸­å…¨ä¼šå®¡è®®é€šè¿‡äº†ã€Šä¸­å…±ä¸­å¤®å…³äºåˆ¶å®šå›½æ°‘ç»æµå’Œç¤¾ä¼šå‘å±•ç¬¬åäº”ä¸ªäº”å¹´è§„åˆ’çš„å»ºè®®ã€‹ï¼Œæ˜ç¡®æå‡ºï¼š\n\n"
            "* æ¨åŠ¨**å»ºè®¾ç°ä»£åŒ–äº§ä¸šä½“ç³»**ï¼Œå·©å›ºå£®å¤§å®ä½“ç»æµæ ¹åŸº\n"
            "* åŠ å¿«å®ç°**é«˜æ°´å¹³ç§‘æŠ€è‡ªç«‹è‡ªå¼º**ï¼Œçªç ´å…³é”®æ ¸å¿ƒæŠ€æœ¯\n"
            "* å…¨é¢æ¨è¿›**æ•°å­—ä¸­å›½å»ºè®¾**ï¼Œå®æ–½\"äººå·¥æ™ºèƒ½+\"è¡ŒåŠ¨\n\n"
            "### æ ¸å¿ƒè¦ç‚¹\n\n"
            "1. **é«˜ç«¯æŠ€æœ¯æ–¹å‘**ï¼šèšç„¦**é›†æˆç”µè·¯ã€å·¥ä¸šæ¯æœºã€é«˜ç«¯ä»ªå™¨**ç­‰é¢†åŸŸå¼€å±•æ”»å…³\n"
            "2. **ç§‘æŠ€ä¸äº§ä¸šèåˆ**ï¼šæ”¯æŒä¼ä¸šç‰µå¤´è”åˆæ”»å…³ï¼ŒåŠ å¼ºæˆæœè½¬åŒ–åº”ç”¨\n"
            "3. **æ•°å­—åŸºç¡€èƒ½åŠ›**ï¼šå¼ºåŒ–**ç®—åŠ›ã€ç®—æ³•ã€æ•°æ®**ç­‰é«˜æ•ˆä¾›ç»™ï¼Œå…¨æ–¹ä½èµ‹èƒ½åƒè¡Œç™¾ä¸š\n\n"
            "> ğŸ“Œ \"åäº”äº”\"è§„åˆ’å»ºè®®å¯¹ä»ªå™¨è£…å¤‡ã€è‡ªä¸»ç ”å‘èƒ½åŠ›ä¸æ™ºèƒ½æŠ€æœ¯æå‡ºç³»ç»Ÿéƒ¨ç½²ï¼Œé‡Šæ”¾æ˜ç¡®æ”¿ç­–å¯¼å‘ã€‚"
        ),
        "priority": 1,
        "updatedAt": "2023-11-15T09:00:00Z",
    },
    {
        "id": 2,
        "title": "è®ºæ–‡æŠ¥å‘Š",
        "icon": "el-icon-reading",
        "content": (
            "## å‰æ²¿ç ”ç©¶è¿›å±•\n\n"
            "### Nanoscale æœ€æ–°å‘è¡¨\n\n"
            "**Temperature-dependent sign reversal of tunneling magnetoresistance in van der Waals ferromagnetic heterojunctions**\n\n"
            "è¥¿å®‰äº¤é€šå¤§å­¦ææ–™ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢è‡ªæ—‹ç”µå­ä¸é‡å­ç³»ç»Ÿç ”ç©¶ä¸­å¿ƒå›¢é˜Ÿäºã€ŠNanoscaleã€‹æœŸåˆŠå‘è¡¨ç ”ç©¶æˆæœï¼Œæ­ç¤ºç£éš§é“ç»“ä¸­TMRä¿¡å·éšæ¸©åº¦å˜åŒ–å‘ç”Ÿæ­£è´Ÿåè½¬çš„ç‰©ç†æœºåˆ¶ï¼š\n\n"
            "* æ„å»º CrVIâ‚† / Feâ‚ƒGeTeâ‚‚ å¼‚è´¨ç»“æ„ï¼Œè§‚å¯Ÿåˆ°å±…é‡Œæ¸©åº¦é™„è¿‘å‡ºç°**TMRç¬¦å·åè½¬**\n"
            "* å®éªŒéªŒè¯**åé“ç£è€¦åˆ**æ˜¯TMRåè½¬çš„æ ¸å¿ƒæœºåˆ¶\n"
            "* å‘ç°**æ¸©åº¦+åå‹è”åŠ¨è°ƒæ§ä¸‹å¯å®ç°å¤šæ€TMRè¡Œä¸º**\n\n"
            "### å®éªŒæ”¯æ’‘è®¾å¤‡\n\n"
            "è¯¥ç ”ç©¶ä¾æ‰˜**è‡´çœŸç²¾å¯†ä»ªå™¨ KMP-L ç³»ç»Ÿ**å®Œæˆå…³é”®æµ‹è¯•ï¼š\n\n"
            "* æˆåŠŸå®ç°**ä½æ¸©å¼ºåœºå¾®åŒºç£ç•´æˆåƒ**\n"
            "* åœ¨50Kä¸‹è§‚å¯Ÿå¼‚è´¨å±‚å‘ˆç°**ç›¸åç£è¡¬åº¦**\n"
            "* ç³»ç»Ÿæä¾›ç©ºé—´åˆ†è¾¨çš„MOKEæµ‹é‡ï¼Œç›´æ¥è¯å®AFMè€¦åˆå­˜åœ¨\n\n"
            "> ğŸ’¡ KMP-Lç³»ç»Ÿæˆä¸ºè¿æ¥ææ–™å¾®è§‚ç£ç»“æ„ä¸å™¨ä»¶å®è§‚æ€§èƒ½çš„é‡è¦çº½å¸¦ï¼Œæ˜¾è‘—æ”¯æ’‘äº†è¯¥ç ”ç©¶æˆæœçš„éªŒè¯è¿‡ç¨‹ã€‚"
        ),
        "priority": 2,
        "updatedAt": "2023-11-15T08:30:00Z",
    },
    {
        "id": 3,
        "title": "å¸‚åœºåŠ¨æ€",
        "icon": "el-icon-data-line",
        "content": (
            "## æœ€æ–°äº§ä¸šåŠ¨æ€\n\n"
            "**å¯Œç¿æ€Ã—å¤©ç‘ç®—å…±å»ºåŸå­åŠ›æ˜¾å¾®é•œåˆ†ææµ‹è¯•ä¸­å¿ƒï¼Œè½åœ°æˆéƒ½**\n\n"
            "2025å¹´9æœˆ25æ—¥ï¼Œå¯Œç¿æ€ä¸å¤©ç‘ç®—åœ¨æˆéƒ½ç­¾ç½²åˆä½œåè®®ï¼Œè”åˆè®¾ç«‹**åŸå­åŠ›æ˜¾å¾®é•œï¼ˆAFMï¼‰åˆ†ææµ‹è¯•ä¸­å¿ƒ**ï¼Œæ—¨åœ¨æ¨åŠ¨é«˜ç«¯ç²¾å¯†æ£€æµ‹èµ„æºæ›´å¥½æœåŠ¡ç§‘ç ”ä¸äº§ä¸šä¸€çº¿ã€‚\n\n"
            "### æ ¸å¿ƒè¦ç‚¹\n\n"
            "1. **åˆä½œå†…å®¹**ï¼šä¸­å¿ƒèšç„¦**å½¢è²Œè¡¨å¾ã€ç‰©æ€§åˆ†æã€å·¥ä¸šçº§æ£€æµ‹æ ¡å‡†**ç­‰æ ¸å¿ƒæ–¹å‘\n"
            "2. **è®¾å¤‡æ”¯æ’‘**ï¼šå¯Œç¿æ€æä¾›**ç§‘ç ”è‡³è®¡é‡çº§**å…¨ç³»åˆ—AFMäº§å“ï¼ŒæŠ€æœ¯æ¶µç›–\"True3Dæ‰«æ\"\"è‡ªåŠ¨æ¢é’ˆç³»ç»Ÿ\"ç­‰\n"
            "3. **æœåŠ¡ä½“ç³»**ï¼šå¤©ç‘ç®—æä¾›**å®éªŒæ£€æµ‹ã€ç®—åŠ›æ”¯æŒä¸å®šåˆ¶åŒ–ç§‘ç ”æœåŠ¡**ï¼Œå½¢æˆ\"è®¾å¤‡+æœåŠ¡\"ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆ\n\n"
            "> ğŸ“Œ æ­¤æ¬¡åˆä½œä¸ºé«˜ç«¯æ£€æµ‹è®¾å¤‡åœ¨ç§‘ç ”ä¸å·¥ç¨‹é¢†åŸŸçš„æ·±å…¥åº”ç”¨æ­å»ºæ–°å¹³å°ï¼ŒåŠ©åŠ›èµ„æºæ•´åˆä¸èƒ½åŠ›æå‡ã€‚"
        ),
        "priority": 3,
        "updatedAt": "2023-11-15T07:00:00Z",
    },
]


def _to_iso(value: Optional[Any]) -> Optional[str]:
    """å°† str/datetime è½¬æˆ ISO8601 UTCï¼ˆæ— å¾®ç§’ï¼‰ï¼›å…¶ä½™ç±»å‹è¿”å› Noneã€‚"""
    if value is None:
        return None
    if isinstance(value, datetime):
        dt = value.astimezone(timezone.utc) if value.tzinfo else value.replace(tzinfo=timezone.utc)
        return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
    if isinstance(value, str):
        txt = value.strip()
        if not txt:
            return None
        try:
            if txt.endswith("Z"):
                txt = txt.replace("Z", "+00:00")
            dt = datetime.fromisoformat(txt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            else:
                dt = dt.astimezone(timezone.utc)
            return dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
        except Exception:
            return value
    return None


def _get_tavily_key() -> str:
    return os.getenv("TAVILY_API_KEY", "").strip()


def _today_key() -> str:
    return datetime.now().strftime("%Y-%m-%d")


def _fetch_cached_report(date_key: str) -> Optional[Tuple[str, List[Dict[str, Any]]]]:
    if not _supabase:
        return None
    try:
        res = (
            _supabase.table(AGENT_REPORT_CACHE_TABLE)
            .select("generated_at, sections, source, updated_at")
            .eq("cache_date", date_key)
            .order("updated_at", desc=True)
            .limit(1)
            .execute()
        )
        rows = res.data or []
        if not rows:
            return None
        row = rows[0]
        sections = row.get("sections")
        if not isinstance(sections, list):
            return None
        generated_at = row.get("generated_at") or row.get("updated_at") or _FALLBACK_GENERATED_AT
        return generated_at, sections
    except Exception as exc:
        print(f"âš ï¸ è¯»å–æ—¥æŠ¥ç¼“å­˜å¤±è´¥: {exc}")
        return None


def _fetch_latest_cached_report() -> Optional[Tuple[str, List[Dict[str, Any]]]]:
    if not _supabase:
        return None
    try:
        res = (
            _supabase.table(AGENT_REPORT_CACHE_TABLE)
            .select("cache_date, generated_at, sections, source, updated_at")
            .order("cache_date", desc=True)
            .order("updated_at", desc=True)
            .limit(1)
            .execute()
        )
        rows = res.data or []
        if not rows:
            return None
        row = rows[0]
        sections = row.get("sections")
        if not isinstance(sections, list):
            return None
        generated_at = row.get("generated_at") or row.get("updated_at") or _FALLBACK_GENERATED_AT
        return generated_at, sections
    except Exception as exc:
        print(f"âš ï¸ è¯»å–æœ€æ–°æ—¥æŠ¥ç¼“å­˜å¤±è´¥: {exc}")
        return None


def _save_cached_report(date_key: str, generated_at: str, sections: List[Dict[str, Any]], source: str) -> None:
    if not _supabase:
        return
    payload = {
        "cache_date": date_key,
        "generated_at": generated_at,
        "sections": sections,
        "source": source,
        "updated_at": _to_iso(datetime.now(timezone.utc)) or _FALLBACK_GENERATED_AT,
    }
    try:
        _supabase.table(AGENT_REPORT_CACHE_TABLE).upsert(payload, on_conflict="cache_date").execute()
    except Exception as exc:
        print(f"âš ï¸ å†™å…¥æ—¥æŠ¥ç¼“å­˜å¤±è´¥: {exc}")


def _normalize_date(value: Optional[Any]) -> Optional[str]:
    if not value:
        return None
    text = str(value).strip()
    if not text:
        return None
    try:
        if text.endswith("Z"):
            text = text.replace("Z", "+00:00")
        dt = datetime.fromisoformat(text)
        return dt.date().isoformat()
    except Exception:
        return text[:10] if len(text) >= 10 else None


def _extract_source_name(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        return parsed.netloc or ""
    except Exception:
        return ""


def _truncate(text: str, limit: int) -> str:
    if not text:
        return ""
    cleaned = " ".join(text.split())
    if len(cleaned) <= limit:
        return cleaned
    return cleaned[:limit].rstrip() + "..."


def _normalize_tavily_result(raw: dict) -> dict:
    title = str(raw.get("title") or "").strip()
    url = str(raw.get("url") or "").strip()
    snippet = (
        str(raw.get("content") or raw.get("snippet") or raw.get("summary") or "").strip()
    )
    published_at = _normalize_date(
        raw.get("published_date")
        or raw.get("published_time")
        or raw.get("published")
        or raw.get("date")
    )
    source = str(raw.get("source") or "").strip() or _extract_source_name(url)
    return {
        "title": title,
        "url": url,
        "snippet": _truncate(snippet, 260),
        "publishedAt": published_at,
        "source": source,
    }


def _dedupe_results(results: List[dict]) -> List[dict]:
    seen = set()
    deduped = []
    for item in results:
        key = item.get("url") or item.get("title")
        if not key or key in seen:
            continue
        seen.add(key)
        deduped.append(item)
    return deduped


def _call_tavily_search(query: str, max_results: int) -> List[dict]:
    api_key = _get_tavily_key()
    if not api_key:
        raise RuntimeError("missing TAVILY_API_KEY")
    payload = {
        "api_key": api_key,
        "query": query,
        "search_depth": AGENT_REPORT_SEARCH_DEPTH,
        "max_results": max_results,
        "include_answer": False,
        "include_images": False,
        "include_raw_content": False,
    }
    timeout = int(os.getenv("TAVILY_TIMEOUT", "25"))
    try:
        response = requests.post("https://api.tavily.com/search", json=payload, timeout=timeout)
        response.raise_for_status()
        data = response.json() or {}
    except requests.RequestException as exc:
        detail = ""
        if getattr(exc, "response", None) is not None:
            try:
                detail = exc.response.text[:300]
            except Exception:
                detail = ""
        print(f"âš ï¸ Tavily è¯·æ±‚å¤±è´¥: {exc} {detail}".strip())
        raise
    raw_results = data.get("results") or []
    normalized = [_normalize_tavily_result(item) for item in raw_results if isinstance(item, dict)]
    normalized = [item for item in normalized if item.get("url") and item.get("title")]
    return _dedupe_results(normalized)


def _llm_summarize_section(heading: str, results: List[dict]) -> str:
    if not results:
        return "- æš‚æ— å¯é å…¬å¼€ä¿¡æ¯ï¼Œå¯ç¨ååˆ·æ–°æŸ¥çœ‹"

    context_lines = []
    for idx, item in enumerate(results[:AGENT_REPORT_SOURCE_LIMIT], start=1):
        title = item.get("title") or "æœªå‘½å"
        snippet = item.get("snippet") or ""
        source = item.get("source") or "æœªçŸ¥æ¥æº"
        published = item.get("publishedAt") or "æœªçŸ¥æ—¥æœŸ"
        url = item.get("url") or ""
        context_lines.append(
            f"{idx}. æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{snippet}\næ¥æºï¼š{source}\nå‘å¸ƒæ—¶é—´ï¼š{published}\né“¾æ¥ï¼š{url}"
        )

    prompt = (
        f"è¯·æ ¹æ®ä¸‹é¢çš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆã€Š{heading}ã€‹ç®€æŠ¥ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) ç”¨ä¸­æ–‡è¾“å‡ºï¼›\n"
        "2) 3-5æ¡è¦ç‚¹ï¼Œä½¿ç”¨é¡¹ç›®ç¬¦å·ï¼›\n"
        "3) æ€»å­—æ•°æ§åˆ¶åœ¨120-220å­—ï¼›\n"
        "4) ä¸è¦æé€ äº‹å®ï¼Œä¸è¦ç¼–é€ æ¥æºï¼›\n"
        "5) ä¸è¦åŒ…å«é“¾æ¥æˆ–å¼•ç”¨æ ¼å¼ã€‚\n\n"
        "æ£€ç´¢ç»“æœï¼š\n"
        + "\n\n".join(context_lines)
    )

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¼ä¸šæƒ…æŠ¥åŠ©æ‰‹ï¼Œè¾“å‡ºç²¾ç‚¼ã€ç»“æ„åŒ–çš„ä¸­æ–‡è¦ç‚¹ã€‚",
        },
        {"role": "user", "content": prompt},
    ]

    try:
        resp = llm.chat(
            messages,
            temperature=0.3,
            top_p=0.8,
            max_tokens=500,
            model=AGENT_REPORT_SUMMARY_MODEL,
            timeout=60,
        )
        data = resp.json() if hasattr(resp, "json") else {}
        content = (
            data.get("choices", [{}])[0]
            .get("message", {})
            .get("content", "")
            .strip()
        )
        if content:
            return content
    except Exception:
        pass

    fallback_lines = [f"- {item.get('title')}" for item in results[:3] if item.get("title")]
    return "\n".join(fallback_lines) or "- æš‚æ— å¯é å…¬å¼€ä¿¡æ¯ï¼Œå¯ç¨ååˆ·æ–°æŸ¥çœ‹"


def _build_dynamic_sections() -> Tuple[str, List[Dict[str, Any]]]:
    generated_at = _to_iso(datetime.now(timezone.utc)) or _FALLBACK_GENERATED_AT
    sections: List[Dict[str, Any]] = []

    for section in SECTION_CONFIGS:
        try:
            results = _call_tavily_search(
                section["query"],
                max_results=AGENT_REPORT_SEARCH_MAX_RESULTS,
            )
        except Exception as exc:
            print(f"âš ï¸ Tavily æœç´¢å¤±è´¥ [{section['title']}]: {exc}")
            results = []
        summary = _llm_summarize_section(section["heading"], results)
        content = f"## {section['heading']}\n\n{summary}"

        sources = []
        for item in results[:AGENT_REPORT_SOURCE_LIMIT]:
            sources.append(
                {
                    "title": item.get("title"),
                    "url": item.get("url"),
                    "source": item.get("source"),
                    "publishedAt": item.get("publishedAt"),
                }
            )

        sections.append(
            {
                "id": section["id"],
                "title": section["title"],
                "icon": section["icon"],
                "content": content,
                "priority": section["priority"],
                "updatedAt": generated_at,
                "sources": sources,
            }
        )

    sections.sort(key=lambda s: (s.get("priority", 99), s.get("id", 0)))
    return generated_at, sections


def _get_cached_or_refresh(force_refresh: bool) -> Tuple[str, List[Dict[str, Any]], str]:
    today = _today_key()
    if not force_refresh:
        with REPORT_CACHE_LOCK:
            cached_date = REPORT_CACHE_STATE.get("date")
            cached_payload = REPORT_CACHE_STATE.get("payload")
            if cached_payload and cached_date == today:
                return cached_payload["generatedAt"], cached_payload["sections"], "cache"

        cached_db = _fetch_cached_report(today)
        if cached_db:
            generated_at, sections = cached_db
            payload = {"generatedAt": generated_at, "sections": sections}
            with REPORT_CACHE_LOCK:
                REPORT_CACHE_STATE["date"] = today
                REPORT_CACHE_STATE["payload"] = payload
            return generated_at, sections, "cache-db"

    try:
        generated_at, sections = _build_dynamic_sections()
    except Exception as exc:
        print(f"âš ï¸ ç”ŸæˆåŠ¨æ€æ—¥æŠ¥å¤±è´¥ï¼Œå›é€€ç¼“å­˜: {exc}")
        with REPORT_CACHE_LOCK:
            cached_payload = REPORT_CACHE_STATE.get("payload")
        if cached_payload:
            return cached_payload["generatedAt"], cached_payload["sections"], "cache-stale"
        cached_db = _fetch_cached_report(today)
        if cached_db:
            return cached_db[0], cached_db[1], "cache-stale"
        cached_latest = _fetch_latest_cached_report()
        if cached_latest:
            return cached_latest[0], cached_latest[1], "cache-stale"
        raise

    payload = {"generatedAt": generated_at, "sections": sections}
    with REPORT_CACHE_LOCK:
        REPORT_CACHE_STATE["date"] = today
        REPORT_CACHE_STATE["payload"] = payload
    _save_cached_report(today, generated_at, sections, "tavily")
    return generated_at, sections, "tavily"


def _fetch_from_supabase(limit: int) -> Tuple[str, List[Dict[str, Any]]]:
    if not _supabase:
        raise RuntimeError("Supabase client not available")

    query = (
        _supabase.table(AGENT_REPORT_SOURCE)
        .select("*")
        .order("priority", desc=False)
        .order("updated_at", desc=True)
    )
    if limit > 0:
        query = query.limit(limit)
    res = query.execute()
    rows = res.data or []

    if not rows:
        raise ValueError("no rows in Supabase result")

    sections: List[Dict[str, Any]] = []
    generated_candidates: List[str] = []

    for row in rows:
        section_id = row.get("id")
        title = row.get("title")
        content = row.get("content")
        icon = row.get("icon")

        if section_id is None or not title or not content:
            continue

        priority_val = row.get("priority")
        try:
            priority_int = int(priority_val)
        except Exception:
            priority_int = 99

        updated_at = row.get("updated_at") or row.get("updatedAt")
        formatted_updated = _to_iso(updated_at) or _to_iso(row.get("created_at"))

        sections.append(
            {
                "id": int(section_id),
                "title": str(title),
                "icon": str(icon) if icon else "el-icon-reading",
                "content": str(content),
                "priority": priority_int,
                "updatedAt": formatted_updated or _FALLBACK_GENERATED_AT,
                "sources": row.get("sources") if isinstance(row.get("sources"), list) else [],
            }
        )

        generated_raw = row.get("generated_at") or row.get("generatedAt")
        formatted_generated = _to_iso(generated_raw)
        if formatted_generated:
            generated_candidates.append(formatted_generated)

    if not sections:
        raise ValueError("no valid sections from Supabase")

    sections.sort(key=lambda s: (s["priority"], s["id"]))

    generated_at = (
        generated_candidates[0]
        if generated_candidates
        else sections[0].get("updatedAt")
        or _FALLBACK_GENERATED_AT
    )

    return generated_at, sections


def _fallback_report() -> Tuple[str, List[Dict[str, Any]]]:
    return _FALLBACK_GENERATED_AT, _FALLBACK_SECTIONS


@agent_report_bp.route("/initial-report", methods=["GET"])
def get_agent_initial_report():
    force_refresh = False
    try:
        from flask import request

        force_refresh = request.args.get("refresh") == "1"
    except Exception:
        force_refresh = False

    tavily_key = _get_tavily_key()
    if not tavily_key:
        print("[WARN] TAVILY_API_KEY missing; Tavily fetch disabled")
    if tavily_key:
        try:
            generated_at, sections, source = _get_cached_or_refresh(force_refresh)
        except Exception:
            try:
                generated_at, sections = _fetch_from_supabase(AGENT_REPORT_LIMIT)
                source = "remote"
            except Exception:
                generated_at, sections = _fallback_report()
                source = "fallback"
    else:
        try:
            generated_at, sections = _fetch_from_supabase(AGENT_REPORT_LIMIT)
            source = "remote"
        except Exception:
            generated_at, sections = _fallback_report()
            source = "fallback"

    payload = {
        "code": 200,
        "message": "success",
        "data": {
            "generatedAt": generated_at,
            "sections": sections,
        },
        "source": source,
    }

    try:
        print(f"[INFO] agent initial report source={source} generatedAt={generated_at}")
    except Exception:
        pass

    response = make_response(json.dumps(payload, ensure_ascii=False, indent=2))
    response.status_code = 200
    response.mimetype = "application/json; charset=utf-8"
    return response


# æ­¤ Blueprint ä»…ä¾› app.py æ³¨å†Œä½¿ç”¨ï¼Œæ— éœ€ç‹¬ç«‹è¿è¡Œå…¥å£
